0
00:00:00,000 --> 00:00:03,450
OK, here we are going to see situations where our nearest

1
00:00:03,450 --> 00:00:05,500
neighbor averaging doesn&#39;t work so well.

2
00:00:05,500 --> 00:00:09,870
And we&#39;re going to have to the find ways to deal with that.

3
00:00:09,870 --> 00:00:12,290
Nearest neighbor averaging, which is the one we just saw,

4
00:00:12,290 --> 00:00:15,330
can be pretty good for small p, small numbers or variables.

5
00:00:15,330 --> 00:00:17,600
Here we had just one variable.

6
00:00:17,600 --> 00:00:22,370
But small, maybe p less than or equal to 4 and largeish N.

7
00:00:22,370 --> 00:00:25,920
Large N so that we have enough points in each neighbor to

8
00:00:25,920 --> 00:00:28,910
average to give us our estimate.

9
00:00:28,910 --> 00:00:32,330
Now this is just one version of a whole class of techniques

10
00:00:32,330 --> 00:00:34,060
called smoothers.

11
00:00:34,060 --> 00:00:38,550
And we&#39;re going to discuss later on in this course much

12
00:00:38,550 --> 00:00:40,880
cleverer ways of doing this kind of averaging such as

13
00:00:40,880 --> 00:00:42,160
kernel and spline smoothing.

14
00:00:46,240 --> 00:00:47,900
Now there&#39;s a problem though.

15
00:00:47,900 --> 00:00:49,790
Nearest neighbor methods can be really

16
00:00:49,790 --> 00:00:51,930
lousy when P is large.

17
00:00:51,930 --> 00:00:54,680
And the reason has got the name the curse of

18
00:00:54,680 --> 00:00:56,270
dimensionality.

19
00:00:56,270 --> 00:00:58,880
What it boils down to is that nearest neighbors tend to be

20
00:00:58,880 --> 00:01:00,575
far away in high dimensions.

21
00:01:03,100 --> 00:01:06,570
So and that creates a problem.

22
00:01:06,570 --> 00:01:10,310
We need to get a reasonable fraction of the N values of yi

23
00:01:10,310 --> 00:01:12,550
to average to bring the variance down.

24
00:01:12,550 --> 00:01:16,000
So we need to average the number of points in each

25
00:01:16,000 --> 00:01:20,820
neighborhood so that our estimate has got a nice,

26
00:01:20,820 --> 00:01:23,680
reasonably small variance.

27
00:01:23,680 --> 00:01:26,430
And let&#39;s suppose we want 10% of the data points to be in

28
00:01:26,430 --> 00:01:28,120
each interval.

29
00:01:28,120 --> 00:01:30,180
The problem is that 10% neighborhood in high

30
00:01:30,180 --> 00:01:32,620
dimensions need no longer be local.

31
00:01:32,620 --> 00:01:35,000
So we loose the spirit of estimating the conditional

32
00:01:35,000 --> 00:01:37,430
expectation by local averaging.

33
00:01:37,430 --> 00:01:40,090
So let&#39;s look at a little example of that.

34
00:01:40,090 --> 00:01:44,510
In the left panel, we&#39;ve got values of two

35
00:01:44,510 --> 00:01:46,620
variables, X1 and X2.

36
00:01:46,620 --> 00:01:49,950
And they are actually uniformly distributed in this

37
00:01:49,950 --> 00:01:55,270
little cube with edges minus 1 to plus 1, minus 1 to plus 1.

38
00:01:55,270 --> 00:01:59,840
And we form two 10% neighborhoods in this case.

39
00:01:59,840 --> 00:02:04,700
The first neighborhood is just involving the variable X1,

40
00:02:04,700 --> 00:02:06,610
ignoring X2.

41
00:02:06,610 --> 00:02:10,250
And so that&#39;s indicated by the vertical dotted lines.

42
00:02:10,250 --> 00:02:13,290
Our target point is at 0.

43
00:02:13,290 --> 00:02:17,010
And so we spread out a neighborhood to the left and

44
00:02:17,010 --> 00:02:20,610
right until we capture 10% of the data points with respect

45
00:02:20,610 --> 00:02:22,480
to the variable X1.

46
00:02:22,480 --> 00:02:26,470
And the dotted line indicates the width of the neighborhood.

47
00:02:26,470 --> 00:02:29,510
Alternatively, if we want to find a neighborhood in two

48
00:02:29,510 --> 00:02:33,420
dimensions, we spread out a circle centered at the target

49
00:02:33,420 --> 00:02:36,320
point, which is the red dot there, until we&#39;ve captured

50
00:02:36,320 --> 00:02:37,990
10% of the points.

51
00:02:37,990 --> 00:02:41,470
Now notice the radius of the circle in two dimensions is

52
00:02:41,470 --> 00:02:45,860
much bigger than the radius of the circle in one dimension

53
00:02:45,860 --> 00:02:47,280
which is just the width between

54
00:02:47,280 --> 00:02:49,440
these two dotted lines.

55
00:02:49,440 --> 00:02:52,390
And so to capture 10% of the points in two dimensions, we

56
00:02:52,390 --> 00:02:56,646
have to go out further and so we are less local than we are

57
00:02:56,646 --> 00:02:58,500
in one dimension.

58
00:02:58,500 --> 00:03:01,150
And so we can take this example further.

59
00:03:01,150 --> 00:03:05,310
And on the right hand plot, I&#39;ve shown you how far you

60
00:03:05,310 --> 00:03:08,900
have to go out in one, two, three, five, and 10

61
00:03:08,900 --> 00:03:10,460
dimensions.

62
00:03:10,460 --> 00:03:13,620
In 10 dimensions, these are different versions of this

63
00:03:13,620 --> 00:03:19,410
problem as the dimensions get higher In order to capture a

64
00:03:19,410 --> 00:03:22,090
certain fraction of the volume.

65
00:03:22,090 --> 00:03:26,270
OK, and so take, for example, 10% or 0.1

66
00:03:26,270 --> 00:03:28,700
fraction of the volume.

67
00:03:28,700 --> 00:03:32,890
So for p equals 1, if the data is uniform, you roughly go out

68
00:03:32,890 --> 00:03:37,420
10% of the distance.

69
00:03:37,420 --> 00:03:40,080
In two dimensions, we store you went more.

70
00:03:40,080 --> 00:03:42,800
Look what happens in five dimensions.

71
00:03:42,800 --> 00:03:47,160
In five dimensions, you have to go out to about 0.9 on each

72
00:03:47,160 --> 00:03:49,800
coordinate axes to get 10% of the data.

73
00:03:49,800 --> 00:03:52,740
That&#39;s just about the whole radius of this sphere.

74
00:03:52,740 --> 00:03:55,840
And in 10 dimensions, you actually have to go break out

75
00:03:55,840 --> 00:03:58,350
of this sphere in order to get points in the corner to

76
00:03:58,350 --> 00:04:00,220
capture the 10%.

77
00:04:00,220 --> 00:04:05,380
So the bottom line here is it&#39;s really hard to find new

78
00:04:05,380 --> 00:04:09,610
neighborhoods in high dimensions and stay local.

79
00:04:09,610 --> 00:04:13,510
If this problem didn&#39;t exist, we would use the near neighbor

80
00:04:13,510 --> 00:04:16,635
averaging as the sole basis for doing estimation.

81
00:04:20,300 --> 00:04:21,950
So how do we deal with this?

82
00:04:21,950 --> 00:04:25,090
Well, we introduce structure to our models.

83
00:04:25,090 --> 00:04:30,090
And so the simplest structural model is a linear model.

84
00:04:30,090 --> 00:04:33,480
And so here we have an example of a linear model.

85
00:04:33,480 --> 00:04:35,750
We&#39;ve got p features.

86
00:04:35,750 --> 00:04:37,950
It&#39;s just got p plus 1 parameters.

87
00:04:37,950 --> 00:04:41,410
And it says the function of X, we can

88
00:04:41,410 --> 00:04:43,650
approximate by a linear function.

89
00:04:43,650 --> 00:04:45,070
So there&#39;s a coefficient on each of

90
00:04:45,070 --> 00:04:46,700
the X&#39;s and at intercept.

91
00:04:46,700 --> 00:04:52,430
So that&#39;s one of the simplest structural models.

92
00:04:52,430 --> 00:04:55,980
We can estimate the parameters of the model by fitting the

93
00:04:55,980 --> 00:04:57,100
model to training data.

94
00:04:57,100 --> 00:05:01,240
And we&#39;ll be talking more about how you do that.

95
00:05:01,240 --> 00:05:04,260
And when we use such a structural model, and, of

96
00:05:04,260 --> 00:05:06,580
course, this structural model is going to avoid the curse of

97
00:05:06,580 --> 00:05:07,410
dimensionality.

98
00:05:07,410 --> 00:05:11,440
Because it&#39;s not relying on any local properties and

99
00:05:11,440 --> 00:05:12,550
nearest neighbor averaging.

100
00:05:12,550 --> 00:05:16,730
That&#39;s just fitting quite a rigid model to all the data.

101
00:05:16,730 --> 00:05:21,470
Now a linear model is almost never correct.

102
00:05:21,470 --> 00:05:24,470
But it often serves as a good approximation, an interpretive

103
00:05:24,470 --> 00:05:31,640
approximation to the unknown true function f of X. So

104
00:05:31,640 --> 00:05:34,830
here&#39;s our same little example data set.

105
00:05:34,830 --> 00:05:38,500
And we can see in the top plot, a linear model gives a

106
00:05:38,500 --> 00:05:41,960
reasonable fit to the data, not perfect.

107
00:05:41,960 --> 00:05:45,130
In the bottom plot, we&#39;ve augmented our linear model.

108
00:05:45,130 --> 00:05:47,490
And we&#39;ve included a quadratic term.

109
00:05:47,490 --> 00:05:52,700
So we put in our X, and we put in an X squared as well.

110
00:05:52,700 --> 00:05:55,660
And we see that fits the data much better.

111
00:05:55,660 --> 00:05:56,760
It&#39;s also a linear model.

112
00:05:56,760 --> 00:06:01,010
But it&#39;s linear in some transformed values of X. And

113
00:06:01,010 --> 00:06:04,870
notice now we&#39;ve put hats on each of the parameters,

114
00:06:04,870 --> 00:06:08,530
suggesting they&#39;ve been estimated, in this case, from

115
00:06:08,530 --> 00:06:09,860
this training data.

116
00:06:09,860 --> 00:06:12,450
These little hats indicate that they&#39;ve been estimated

117
00:06:12,450 --> 00:06:15,070
from the training data.

118
00:06:15,070 --> 00:06:19,780
So those are two parametric models, structured models that

119
00:06:19,780 --> 00:06:21,770
seem to do a good job in this case.

120
00:06:24,340 --> 00:06:27,080
Here&#39;s a two dimensional example.

121
00:06:27,080 --> 00:06:31,180
Again, seniority, years of education, and income.

122
00:06:31,180 --> 00:06:33,390
And this is simulated data.

123
00:06:33,390 --> 00:06:37,060
And so the blue surface is actually showing you the true

124
00:06:37,060 --> 00:06:39,566
function from which the data was simulated with errors.

125
00:06:39,566 --> 00:06:43,000
We can see the errors aren&#39;t big.

126
00:06:43,000 --> 00:06:46,610
Each of those data points comes from a particular pair

127
00:06:46,610 --> 00:06:50,480
of years of education and seniority with some error.

128
00:06:50,480 --> 00:06:53,380
And the little line segments in the data points

129
00:06:53,380 --> 00:06:54,620
show you the error.

130
00:06:54,620 --> 00:06:55,500
OK.

131
00:06:55,500 --> 00:06:59,200
so we can write that as income as a function of education and

132
00:06:59,200 --> 00:07:01,340
seniority plus some error.

133
00:07:01,340 --> 00:07:02,500
So this is with truth.

134
00:07:02,500 --> 00:07:04,960
We actually know this in this case.

135
00:07:04,960 --> 00:07:07,700
And here is a linear regression model fit to those

136
00:07:07,700 --> 00:07:09,490
simulation data.

137
00:07:09,490 --> 00:07:12,200
So it&#39;s an approximation.

138
00:07:12,200 --> 00:07:17,120
It captures the important elements of the relationship

139
00:07:17,120 --> 00:07:19,140
but doesn&#39;t capture everything.

140
00:07:19,140 --> 00:07:19,440
OK.

141
00:07:19,440 --> 00:07:20,690
It&#39;s got three parameters.

142
00:07:23,230 --> 00:07:26,730
Here&#39;s a more flexible regression model.

143
00:07:26,730 --> 00:07:28,560
We&#39;ve actually fit this using a technique

144
00:07:28,560 --> 00:07:30,650
called thin plate splines.

145
00:07:30,650 --> 00:07:34,870
And that&#39;s a nice smooth version of a

146
00:07:34,870 --> 00:07:37,590
two dimensional smoother.

147
00:07:37,590 --> 00:07:40,300
It&#39;s different from nearest neighbor averaging.

148
00:07:40,300 --> 00:07:42,600
It&#39;s got some nicer properties.

149
00:07:42,600 --> 00:07:44,570
And you can see, this does a pretty good job.

150
00:07:44,570 --> 00:07:49,000
If we go back to the generating data and the

151
00:07:49,000 --> 00:07:54,140
generating surface, this thin plate spline actually captures

152
00:07:54,140 --> 00:07:57,150
more of the essence of what&#39;s going on there.

153
00:07:57,150 --> 00:08:00,290
And for thin plate splines, we&#39;re going to talk about them

154
00:08:00,290 --> 00:08:02,030
later in Chapter 7.

155
00:08:02,030 --> 00:08:06,560
There&#39;s a tuning parameter that controls how smooth the

156
00:08:06,560 --> 00:08:08,190
surfaces is.

157
00:08:08,190 --> 00:08:11,000
Here&#39;s an example of a thin plate spline.

158
00:08:11,000 --> 00:08:13,075
We basically tuned the parameter all

159
00:08:13,075 --> 00:08:15,300
the way down to 0.

160
00:08:15,300 --> 00:08:17,840
And this surface actually goes through

161
00:08:17,840 --> 00:08:20,290
every single data point.

162
00:08:20,290 --> 00:08:22,860
In this case, that&#39;s overfitting.

163
00:08:22,860 --> 00:08:24,010
the data.

164
00:08:24,010 --> 00:08:25,300
We expect to have some errors.

165
00:08:25,300 --> 00:08:28,110
Because with true function generate

166
00:08:28,110 --> 00:08:29,720
data points with errors.

167
00:08:29,720 --> 00:08:30,880
So this is known as overfitting.

168
00:08:30,880 --> 00:08:35,090
We are overfitting the training data.

169
00:08:35,090 --> 00:08:37,640
So this is an example where we&#39;ve got a family of

170
00:08:37,640 --> 00:08:40,860
functions, and we&#39;ve got a way of controlling the complexity.

171
00:08:43,760 --> 00:08:48,200
So there&#39;s some trade offs when building models.

172
00:08:48,200 --> 00:08:50,770
One trade off is prediction accuracy versus

173
00:08:50,770 --> 00:08:52,320
interpretability.

174
00:08:52,320 --> 00:08:54,590
So linear models are easy to interpret.

175
00:08:54,590 --> 00:08:56,370
We&#39;ve just got a few parameters.

176
00:08:56,370 --> 00:08:58,270
Thin plate splines are not.

177
00:08:58,270 --> 00:09:00,310
They give you a whole surface back.

178
00:09:00,310 --> 00:09:05,600
And if given a surface back in 10 dimensions, it&#39;s hard to

179
00:09:05,600 --> 00:09:07,850
understand what it&#39;s actually telling you.

180
00:09:10,610 --> 00:09:15,310
We can have a good fit versus over-fit or under-fit.

181
00:09:15,310 --> 00:09:17,820
So in this previous example, the middle one was a good

182
00:09:17,820 --> 00:09:20,300
fit.The linear was slightly under-fit.

183
00:09:20,300 --> 00:09:22,950
And the last one was over-fit.

184
00:09:22,950 --> 00:09:25,790
So how do we know when the fit is just right?

185
00:09:25,790 --> 00:09:30,920
We need to be able to select amongst those.

186
00:09:30,920 --> 00:09:33,590
Parsimony versus black-box.

187
00:09:33,590 --> 00:09:40,240
Parsimony means having a model that&#39;s simpler and can be

188
00:09:40,240 --> 00:09:44,020
transmitted with a small number of parameters and

189
00:09:44,020 --> 00:09:47,500
explained in a simple fashion, involved, maybe, in a subset

190
00:09:47,500 --> 00:09:48,750
of the predictors.

191
00:09:51,040 --> 00:09:55,810
And so those models if they do as well as say a black-box

192
00:09:55,810 --> 00:09:58,180
predictor, like the thin plate spline was somewhat of a

193
00:09:58,180 --> 00:09:59,440
black-box predictor.

194
00:09:59,440 --> 00:10:03,300
We&#39;d prefer the simpler model.

195
00:10:03,300 --> 00:10:07,900
Here&#39;s a little schematic which shows a variety of the

196
00:10:07,900 --> 00:10:11,800
methods that we are going to be discussing in this course.

197
00:10:11,800 --> 00:10:14,310
And they are ordered by interpretability and

198
00:10:14,310 --> 00:10:16,380
flexibility.

199
00:10:16,380 --> 00:10:22,290
And at the top, there are two versions of linear regression,

200
00:10:22,290 --> 00:10:25,740
subset selection and lasso, which we&#39;ll talk about, that

201
00:10:25,740 --> 00:10:29,400
actually even think the linear regression models too complex

202
00:10:29,400 --> 00:10:31,970
and try and reduce it by throwing

203
00:10:31,970 --> 00:10:34,660
out some of the variables.

204
00:10:34,660 --> 00:10:38,200
Linear models and least squares.

205
00:10:38,200 --> 00:10:43,100
Slightly more flexible, but you loose some

206
00:10:43,100 --> 00:10:44,550
interpretability because now all the

207
00:10:44,550 --> 00:10:46,490
variables are thrown in.

208
00:10:46,490 --> 00:10:49,580
Then we have generalize additive models which allow

209
00:10:49,580 --> 00:10:52,130
for transformations in an automatic way

210
00:10:52,130 --> 00:10:53,800
of each of the variables.

211
00:10:53,800 --> 00:10:56,870
And then at the high flexibility, low

212
00:10:56,870 --> 00:10:59,630
interoperability, and bagging, boosting, and

213
00:10:59,630 --> 00:11:01,380
support vector machines.

214
00:11:01,380 --> 00:11:03,620
We&#39;ll discuss all these techniques but

215
00:11:03,620 --> 00:11:06,500
later on in the course.

216
00:11:06,500 --> 00:11:08,560
OK, so we covered linear regression.

217
00:11:08,560 --> 00:11:10,960
And we covered nearest neighbor averaging.

218
00:11:10,960 --> 00:11:14,520
And we talked about ways, places where that&#39;s

219
00:11:14,520 --> 00:11:15,550
not going to work.

220
00:11:15,550 --> 00:11:19,120
And so we&#39;ve briefly introduced a number of other

221
00:11:19,120 --> 00:11:19,900
different methods.

222
00:11:19,900 --> 00:11:24,070
And they are all listed on the screen, different methods that

223
00:11:24,070 --> 00:11:26,980
they can use to solve the problem when the dimensions

224
00:11:26,980 --> 00:11:31,300
are high and when linearity doesn&#39;t work.

225
00:11:31,300 --> 00:11:33,430
But we have to choose amongst these methods.

226
00:11:33,430 --> 00:11:37,020
And so we need to develop ways of making those choices.

227
00:11:37,020 --> 00:11:39,260
And that&#39;s what we&#39;re going to cover in the next segment.

