{"cells":[{"metadata":{"_uuid":"4885a1792a9bead04c84deaabd963b7362ff84df","_execution_state":"idle","trusted":true},"cell_type":"code","source":"## Importing packages\n\n# This R environment comes with all of CRAN and many other helpful packages preinstalled.\n# You can see which packages are installed by checking out the kaggle/rstats docker image: \n# https://github.com/kaggle/docker-rstats\n\nlibrary(tidyverse) # metapackage with lots of helpful functions\n\n## Running code\n\n# In a notebook, you can run a single code cell by clicking in the cell and then hitting \n# the blue arrow to the left, or by clicking in the cell and pressing Shift+Enter. In a script, \n# you can run code by highlighting the code you want to run and then clicking the blue arrow\n# at the bottom of this window.\n\n## Reading in files\n\n# You can access files from datasets you've added to this kernel in the \"../input/\" directory.\n# You can see the files added to this kernel by running the code below. \n\nlist.files(path = \"../input\")\n\n## Saving data\n\n# If you save any files or images, these will be put in the \"output\" directory. You \n# can see the output directory by committing and running your kernel (using the \n# Commit & Run button) and then checking out the compiled version of your kernel.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00762a6c1101d6abe74eaf4555fa2e25a9f8c36e"},"cell_type":"code","source":"#\n# Logistic Regression\n# https://youtu.be/TxvEVc8YNlU\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a774c26c132e6f3ca9331e0eafa08c33fadb17aa"},"cell_type":"code","source":"# Here we use the command require, which is similar to library. \n# I tend to use require. It's sort of more evocative \nrequire(ISLR)\n#?require","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfef861c14f370e51cc0a9b6dae49159dbe1aebd"},"cell_type":"code","source":"# names is useful for seeing what's on the data frame\nnames(Smarket)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa7f8a478ec59d1f0ff89de19fa04f1d27914004"},"cell_type":"code","source":"# Summary gives you a simple summary of each of the variables on the Smarket data frame.\nsummary(Smarket)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1f3f4ba743ac4d0135f2a77020b3de8b20e589c"},"cell_type":"code","source":"# And we can also do help on these data objects and get some details of each of the variables.\n#?Smarket\n\n# So we're going to use the direction as a response and\n# see if we can predict it as a binary response using logistic regression.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94d9f796ba8eda6bfab222cdc19901993b7816fd"},"cell_type":"code","source":"# And we told it to use as the color indicator, actually our binary response.\n# And that's a useful way, when you've got a two-class variable for seeing in the plots \n# which are members of each class.\npairs(Smarket,col=Smarket$Direction)\n#?pairs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32b62fd9a218af80ba9703b72910910a3b9189ed"},"cell_type":"code","source":"# first 5 elements\nSmarket$Direction[1:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"137f416119489da402d54c2d9b5c03c701f2df41"},"cell_type":"code","source":"# Logistic regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aa43d90162cf7330af51319b675983ae55c8528"},"cell_type":"code","source":"# And so that tells GLM to put fit a logistic regression\n# model instead of one of the many other models that can be\n# fit to the GLM.\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial)\n\nglm.fit\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94286a0969e902c9eb4b7122f0fd0adbbb285e39"},"cell_type":"code","source":"summary(glm.fit)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efa2a738e60de0844f3834eb4eaa2631b7f82745"},"cell_type":"code","source":"# And it seems like none of the coefficients are significant here.\n# Again, not a big surprise for these kinds of data.\n# It doesn't necessarily mean it won't be able to make any kind of reasonable predictions.\n# It just means that possibly these variables are very correlated.\n# Actually, the plot doesn't suggest that. \n# Anyway, none are significant.\n\n# And it gives the null deviance, which is the deviance just for the mean.\n# So that's the log likelihood if you just use the mean model, \n# and then the deviance for the model with all the predictors in, \n# that's the residual deviance.\n# And there was a very modest change in deviance.\n# It looks like four units on six degrees of freedom \n\n# => by involving 6 degrees of freedom (log1 log2 log3 log4 log5 volume)\n# , the deviance only reduced by 4 units","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de06e4c51b473541d2231bce4f35a96fba7d90f3"},"cell_type":"code","source":"# https://stats.stackexchange.com/questions/108995/interpreting-residual-and-null-deviance-in-glm-r\n# If your Null Deviance is really small, it means that the Null Model explains \n# the data pretty well. Likewise with your Residual Deviance.\n# you should see that the degrees of freedom reported on the Null are always \n# higher than the degrees of freedom reported on the Residual. \n# That is because :\n# Null Deviance df = Saturated df - Null df = n-1 \n# Residual Deviance df = Saturated df - Proposed df = n-(p+1)\n\n# https://www.theanalysisfactor.com/r-glm-model-fit/\n# deviance: it’s a measure of \"badness\" of fit–higher numbers indicate worse fit.\n# R reports two forms of deviance – the null deviance and the residual deviance.\n# The null deviance shows how well the response variable is predicted by a model\n# that includes only the intercept (grand mean).\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"145386c31847e311e4efe5085ceb64437129fa93"},"cell_type":"code","source":"# we can make predictions from the fitted model.\n# And so we assign to glm.probs the predict of glm.fit, and we\n# tell it type equals response.\nglm.probs=predict(glm.fit,type=\"response\") \nglm.probs[1:5]\n# And it gives you a vector of fitted probabilities.\n# We can look at the first five, and we see that they're very\n# close to 50%, which is, again, not too surprising.\n# We don't expect to get strong predictions in this case.\n# So this is a prediction of whether the market's going to\n# be up or down based on the lags and the other predictors.\n#?predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1192c63ca35f2a5e988b75ae98affb18c5a024c5"},"cell_type":"code","source":"# We can turn those probabilities into classifications by \n# thresholding at 0.5. And so we do that by using the if/else command.\n# So if/else takes effect, in this case, glm.probs, a vector of logicals.\n# So glm.probs bigger than 0.5.\n# So that'll be a vector of trues and falses.\n# And then if/else says that, element by element, if it's\n# true, you're going to call it up.\n# Otherwise, you're going to call it down.\nglm.pred=ifelse(glm.probs>0.5,\"Up\",\"Down\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21261d498edd181761a89921420b5c16d5a262b5"},"cell_type":"code","source":"attach(Smarket)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b1b82fcbda39646eebce84a3785e449c1335de5"},"cell_type":"code","source":"# And now we're going to look at our performance.\n# And now we can make a table of glm.pred, which is our ups and downs \n# from our prediction, against the true direction.\ntable(glm.pred,Direction)\n\n# And we get a table, and we see there's lots of elements on the off diagonals.\n# On the diagonals is where we do correct classification, and\n# on the off diagonals is where we make mistakes.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8391baadd4c0a02771f05fb21b70b33460c404c1"},"cell_type":"code","source":"# And we can actually get our mean classification performance.\n# So that's cases where glm.pred is equal to the direction.\n# And we just take the mean of those, so it'll give you a\n# proportion, in this case, 0.52.\nmean(glm.pred==Direction)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8af4d875699e359de64f811602647cd8feb0bf76"},"cell_type":"code","source":"# Well, we may have overfit on the training data.\n# So what we're going to do now is divide our data up into a\n# training and test set.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a9785a129c535ddf419b5c404c7542ce6402866"},"cell_type":"code","source":"# So what we'll do is we'll make a vector of logicals.\n# And what it is is train is equal to year less than 2005.\n# For all those observations for which year is less than 2005,\n# we'll get a true. Otherwise, we'll get a false.\ntrain = Year<2005\n\n# look at the first 10 elements\ntrain[1:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"399fbe0d5795249d89151aca6ec06e520f3c8c28"},"cell_type":"code","source":"# And now we refit our glm.fit, except we say subset equals train.\n# And so it will use any those observations for which train is true.\n# So now that means we fit just to the data in years less than 2005.\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial, subset=train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb4216d428eb198364b72665176a3c56a8cdc507"},"cell_type":"code","source":"# And now, when we come to predict, we're going to predict on the remaining data,\n# which is years 2005 or greater.\n# And so we use the predict function again.\n# And for the new data, we give it Smarket, but index by not trained.\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b72dc3d9e2aad05faf97dcda337b01833430260c"},"cell_type":"code","source":"glm.pred=ifelse(glm.probs >0.5,\"Up\",\"Down\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"972e2cd592b52939aa07033eb07582ade83dfb54"},"cell_type":"code","source":"# And let's make a subset, a new variable, direction.2005,\n# for the test data, which is the response\n# variable, direction, which is just for our test data.\n# In other words, not trained.\nDirection.2005=Smarket$Direction[!train]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f80a51b00d8230a68319df322fa2cef9dd65277b"},"cell_type":"code","source":"# So now this is on test data.\ntable(glm.pred,Direction.2005)\nmean(glm.pred==Direction.2005)\n\n# So now we actually get slightly less than 50%.\n# So we're doing worse than the null rate, which is 50%.\n# Well, we might be overfitting. And that's why we're doing worse on the test data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6fea77cecf53467e84290d8230675ead0aaa2ba"},"cell_type":"code","source":"# So now we're going to fit a smaller model.\n# So we're going to just use lag1 and lag2 and leave out\n# all the other variables. The rest of it calls the same.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77ff85293763d008e736a5e8ca64c23c2bb98521"},"cell_type":"code","source":"#Fit smaller model\nglm.fit=glm(Direction~Lag1+Lag2,\n            data=Smarket,family=binomial, subset=train)\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \nglm.pred=ifelse(glm.probs >0.5,\"Up\",\"Down\")\ntable(glm.pred,Direction.2005)\nmean(glm.pred==Direction.2005)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bf321798cd615ff807baeaba33a12556c91d96a"},"cell_type":"code","source":"# we get a correct classification of just\n# about 56%???, which is not too bad at all.\n106/(76+106)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"305adcbedad1807e71853eff682bb569271cfd19"},"cell_type":"code","source":"# And so using the smaller model, it appears to have done better here.\n# And if we do a summary of that guy, let's see if anything\n# became significant by using the smaller model, given that\n# it gave us better predictions.\nsummary(glm.fit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0f06edd8b34d97d93d3a6255581372ee727151a"},"cell_type":"code","source":"#\n# Linear Discriminant Analysis\n# https://youtu.be/2cl7JiPzkBY\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c360e3ad9ce8f19a89d4b1e136cf4efc4c484a0e"},"cell_type":"code","source":"require(ISLR)\nrequire(MASS)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74d98612934d6ff6ad10639b6e9d2d3591a68941"},"cell_type":"code","source":"## Linear Discriminant Analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74f6174be4d714eb1c3a3c3825128bae66b5b34c"},"cell_type":"code","source":"?lda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dc198fbf4544230985757ee6b9be089115d61ad"},"cell_type":"code","source":"# And we're going to use the subset, which is years less than 2005, \n# because later on, we're going to make predictions for year 2005.\n# So you can put that directly in the formula, subset equals year less than 2005.\nlda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79e92c803e2a00c390fc142fc4a917c962a8a70b"},"cell_type":"code","source":"# And it fits it so very quickly, and we print it by just typing its name.\nlda.fit\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adbadf4d7be141462c1a04a9064da19c6a5db7d1"},"cell_type":"code","source":"# So the prior probabilities are just the proportions of ups and downs in the data set.\n# It's roughly 50%, which says something about the market, It's kind of a random walk.\n\n# It summarizes the group means for the two groups, for the downs and the ups.\n# It looks like there may be a slight difference in these two groups.\n\n# And then it gives the LDA coefficients. \n# So if you remember the LDA function fits a linear function for separating the two groups.\n# And so, it's got two coefficients.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed0d9b5b3dc8558777ff9f83ccd104f3dc945421"},"cell_type":"code","source":"# https://stats.stackexchange.com/questions/87479/what-are-coefficients-of-linear-discriminants-in-lda\n# LDA has 2 distinct stages: extraction and classification. \n# At extraction, latent variables called discriminants are formed, \n# as linear combinations of the input variables. \n# The coefficients in that linear combinations are called discriminant coefficients; these are what you ask about. \n# On the 2nd stage, data points are assigned to classes by those discriminants, \n# not by original variables.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1860d949b3acd6c5b8d944fc23b830dfec8bbda"},"cell_type":"code","source":"lda.fit[1:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af502eaf34b9a7169387805b2aed3548da92cd28"},"cell_type":"code","source":"# It plots a linear discriminant function separately, \n# the values of the linear discriminant function,\n# separately for the up group and the down group.\n# And when we look at this, it looks to the eye like there's really not much difference.\nplot(lda.fit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1be50441dc5ea572bbd88414c23b459450b3e021"},"cell_type":"code","source":"# So now we're going to see how well our rule predicts on the year 2005\n# I'm doing this in a slightly different way \n\n# And we use a command in R-- a useful command-- called subset.\n# And so the first argument is the data frame that you're going to subset, \n# which is s market. And then following that are some logical expressions that\n# can use variables in that data frame to define the subset.\n# And that will create a data frame with just the 2005 observations.\nSmarket.2005=subset(Smarket,Year==2005)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"471ccdeacb4e4b50ad0d85ba2ba520607c6e9a0f"},"cell_type":"code","source":"# And so now we can use that as the test data, or the place where we want to make our predictions.\nlda.pred=predict(lda.fit,Smarket.2005)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66afc0e0f17ae1cf30a60d1cd72b88addec48466"},"cell_type":"code","source":"# print the first 5 of these\nlda.pred[1:5,]\n# So I was assuming it was in a matrix format, and it's not.\n# So what format is it?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"665f11460813c63ee6b035d41b6618a85cf2afc9"},"cell_type":"code","source":"class(lda.pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc73a880952045d1e4a3950e22332cc4cef476d0"},"cell_type":"code","source":"# And when you have a list of variables, and each of the\n# variables have the same number of observations, a convenient\n# way of looking at such a list is through data frame.\ndata.frame(lda.pred)[1:5,]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97e9f36216e83c984cf9fe3dc20b553b6c72a680"},"cell_type":"code","source":"# The thing we're really interested in here is the classification,\n# We'll do a table of that, and we get the little confusion matrix\ntable(lda.pred$class,Smarket.2005$Direction)\n\n# ones, we can just take the mean of that, and it'll give\n# us our current classification rate, which in this case is about 0.56.\nmean(lda.pred$class==Smarket.2005$Direction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9cece1d654254407ca17ddcf876a1dab96e52a9"},"cell_type":"code","source":"#\n# Nearest Neighbor Classification\n# https://youtu.be/9TVVF7CS3F4\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27c30719a49bf8b00faf26f556a5e564ff12a9e9"},"cell_type":"code","source":"## K-Nearest Neighbors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e38e01ee233b69d50decfc15bbdf00e3b7f68dd"},"cell_type":"code","source":"# This time, we're going to look at k-nearest neighbor classification, \n# which it's one of those tools that's a verysimple classification rule, \n# but it's effective a lot of the time.\n# Some experts have written that k-nearest neighbors do the best about one third of the time.\n# And it's so simple that, in the game of doing classification, \n# you always want to have k-nearest neighbors in your toolbox.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af49ad68b444e87f12826e39047088cd389a3bc5"},"cell_type":"code","source":"library(class)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da31da963acb685c53952db95199910e221e8791"},"cell_type":"code","source":"#?knn\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d98b6a50369a569b3db5ec72c26668574e08de6a"},"cell_type":"code","source":"attach(Smarket)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02ff62071442d5ff7eecc0ddcdb3c90aab71abb3"},"cell_type":"code","source":"# So what we'll do is we'll make a matrix of lag1 and lag2.\nXlag=cbind(Lag1,Lag2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d99d11ea8255744b8fedf5d6fb498aa6ad0a4cce"},"cell_type":"code","source":"# let's look at the first five rows of that matrix.\nXlag[1:5,]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"857ceddc16fdff84dda28e2c6d8ebf04198243db"},"cell_type":"code","source":"# And then we'll make a indicator variable Train which is year less than 2005.\ntrain=Year<2005\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20a59a0bb36b8f01c4a02dc40f34d43a837ffbce"},"cell_type":"code","source":"# we're ready to call k and n, so we give our matrix x lag, \n# and right in line there we index it by Train which is just using the training observations.\n# And then, for the test observations, we give it x lag not Train.\n# So those not train will be, therefore, those that are equal to 2005\n# k=1:\n# that means what the algorithm does is, \n# it says to classify a new observation,\n# you go into the training set in the x space, the feature space, \n# and you look for the training observation that's \n# closest to your test point in Euclidean distance and you classify to its class.\nknn.pred=knn(Xlag[train,],Xlag[!train,],Direction[train],k=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0af63508be78e98a623e5a77a276ab66cee9a9f"},"cell_type":"code","source":"table(knn.pred,Direction[!train])\n# so its useless ... ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"707fd0129a446e87c297c66f4abe3d4a44452448"},"cell_type":"code","source":"mean(knn.pred==Direction[!train])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"538ae4571b2c8a35389d9dae6833ea6f3e54839f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}