0
00:00:00,000 --> 00:00:03,790
OK, we&#39;re going to talk about statistical learning and

1
00:00:03,790 --> 00:00:05,420
models now.

2
00:00:05,420 --> 00:00:09,220
I&#39;m going to tell you what models of good for, how we use

3
00:00:09,220 --> 00:00:12,630
them, and what are some of the issues involved.

4
00:00:12,630 --> 00:00:16,040
OK so we see three plots in front of us.

5
00:00:16,040 --> 00:00:19,970
These are sales figures from a marketing campaign as a

6
00:00:19,970 --> 00:00:24,150
function of amount spent on TV ads, radio ads,

7
00:00:24,150 --> 00:00:26,050
and newspaper ads.

8
00:00:26,050 --> 00:00:30,660
And you can see at least in the first two there&#39;s somewhat

9
00:00:30,660 --> 00:00:31,870
of a trend.

10
00:00:31,870 --> 00:00:36,480
And, in fact, we&#39;ve summarized the trend by a little linear

11
00:00:36,480 --> 00:00:38,470
regression line in each.

12
00:00:38,470 --> 00:00:41,570
And so we see that there&#39;s some relationship.

13
00:00:41,570 --> 00:00:46,270
The first two, again, look stronger than the third.

14
00:00:46,270 --> 00:00:50,300
Now, in a situation like this, we typically like to know the

15
00:00:50,300 --> 00:00:56,090
joint relationship between the response sales and all three

16
00:00:56,090 --> 00:00:57,970
of these together.

17
00:00:57,970 --> 00:01:01,260
We want to understand how they operate together

18
00:01:01,260 --> 00:01:03,620
to influence sales.

19
00:01:03,620 --> 00:01:09,440
So you can think of that as wanting to model sales as a

20
00:01:09,440 --> 00:01:12,760
function of TV, radio, and

21
00:01:12,760 --> 00:01:15,040
newspaper all jointly together.

22
00:01:15,040 --> 00:01:18,450
So how do we do that?

23
00:01:18,450 --> 00:01:22,170
So before we get into the details

24
00:01:22,170 --> 00:01:24,300
let&#39;s set up some notation.

25
00:01:24,300 --> 00:01:27,390
So here, sales is the response, or the target, that

26
00:01:27,390 --> 00:01:29,800
we wish to predict or model.

27
00:01:29,800 --> 00:01:34,270
And we usually refer to it as Y. We use the letter Y to

28
00:01:34,270 --> 00:01:35,460
refer to it.

29
00:01:35,460 --> 00:01:39,880
TV is one of the features or inputs or predictors, and

30
00:01:39,880 --> 00:01:41,470
we&#39;ll call it X1.

31
00:01:41,470 --> 00:01:45,650
Likewise, radio is X2 and so on.

32
00:01:45,650 --> 00:01:48,200
So in this case, we&#39;ve got three predictors, and we can

33
00:01:48,200 --> 00:01:53,540
refer to them collectively by a vector as X equal to, with

34
00:01:53,540 --> 00:01:55,910
three components X1, X2, and X3.

35
00:01:55,910 --> 00:01:59,220
And vectors we generally think of as column vectors.

36
00:01:59,220 --> 00:02:01,930
And so that&#39;s a little bit of notation.

37
00:02:01,930 --> 00:02:05,320
And so now in this small compact notation, we can write

38
00:02:05,320 --> 00:02:11,510
our model as Y equals function of X plus error.

39
00:02:11,510 --> 00:02:16,080
OK and this error, it&#39;s just a catch all.

40
00:02:16,080 --> 00:02:19,890
It captures the measurement errors maybe in Y and other

41
00:02:19,890 --> 00:02:20,550
discrepancies.

42
00:02:20,550 --> 00:02:23,620
Our function of X is never going to model Y perfectly.

43
00:02:23,620 --> 00:02:26,320
So there&#39;s going to be a lot of things we can&#39;t capture

44
00:02:26,320 --> 00:02:27,080
with the function.

45
00:02:27,080 --> 00:02:29,210
And that&#39;s caught up in the error.

46
00:02:29,210 --> 00:02:34,620
And, again, f of X here is now a function of this vector X

47
00:02:34,620 --> 00:02:40,290
which has these three arguments, three components.

48
00:02:40,290 --> 00:02:43,590
So what is the function f of X good for?

49
00:02:43,590 --> 00:02:47,200
So with a good f, we can make predictions of what new points

50
00:02:47,200 --> 00:02:48,990
X equals to little x.

51
00:02:48,990 --> 00:02:52,190
So this notation capital X equals little x.

52
00:02:52,190 --> 00:02:54,800
You know, capital X, we think as the variable, having these

53
00:02:54,800 --> 00:02:56,380
three components.

54
00:02:56,380 --> 00:03:00,070
And little x is an instance also with three components,

55
00:03:00,070 --> 00:03:06,590
particular values for newspaper, radio, and TV.

56
00:03:06,590 --> 00:03:08,160
With the model we can understand which

57
00:03:08,160 --> 00:03:09,860
components of X--

58
00:03:09,860 --> 00:03:12,040
in general, it&#39;ll have P components, if there&#39;s P

59
00:03:12,040 --> 00:03:12,670
predictors--

60
00:03:12,670 --> 00:03:16,690
are important in explaining Y, and which are irrelevant.

61
00:03:16,690 --> 00:03:19,640
For example, if we model in income as a function of

62
00:03:19,640 --> 00:03:22,600
demographic variables, seniority and years of

63
00:03:22,600 --> 00:03:25,370
education might have a big impact on income, but marital

64
00:03:25,370 --> 00:03:26,910
status typically does not.

65
00:03:26,910 --> 00:03:30,280
And we&#39;d like our model to be able to tell us that.

66
00:03:30,280 --> 00:03:32,470
And depending on the complexity of f, we may be

67
00:03:32,470 --> 00:03:36,530
able to understand how each component Xj affects Y, in

68
00:03:36,530 --> 00:03:41,270
what particular fashion it affects Y. So models have many

69
00:03:41,270 --> 00:03:44,560
uses and those amongst them.

70
00:03:44,560 --> 00:03:46,920
OK, well, what is this function f?

71
00:03:46,920 --> 00:03:49,860
And is there an ideal f?

72
00:03:49,860 --> 00:03:55,830
So in the plot, we&#39;ve got a large sample of points from a

73
00:03:55,830 --> 00:03:56,700
population.

74
00:03:56,700 --> 00:04:00,430
There is just a single X in this case and a response Y.

75
00:04:00,430 --> 00:04:03,860
And you can see, it&#39;s a scatter plot, so we see

76
00:04:03,860 --> 00:04:05,970
there&#39;s a lot of points.

77
00:04:05,970 --> 00:04:07,560
There is 2,000 points here.

78
00:04:07,560 --> 00:04:10,070
Let&#39;s think of this as actually the whole population

79
00:04:10,070 --> 00:04:13,930
or rather as a representation of a very large population.

80
00:04:16,560 --> 00:04:20,130
And so now let&#39;s think of what a good function f might be.

81
00:04:20,130 --> 00:04:22,910
And let&#39;s say not just the whole function, but let&#39;s

82
00:04:22,910 --> 00:04:26,360
think what value would we like f to have at say the value of

83
00:04:26,360 --> 00:04:27,910
X equals 4.

84
00:04:27,910 --> 00:04:30,150
So at this point over here.

85
00:04:30,150 --> 00:04:34,050
We want to query f at all values of X. But we are

86
00:04:34,050 --> 00:04:36,910
wondering what it should be at the value 4.

87
00:04:36,910 --> 00:04:40,110
So you&#39;ll notice that at the X equals 4, there are many

88
00:04:40,110 --> 00:04:45,330
values of Y. But a function can only take on one value.

89
00:04:45,330 --> 00:04:47,400
The function is going to deliver back one value.

90
00:04:47,400 --> 00:04:49,160
So what is a good value?

91
00:04:49,160 --> 00:04:53,690
Well, one good value is to deliver back the average

92
00:04:53,690 --> 00:04:57,540
values of those Y&#39;s who have X equal to 4.

93
00:04:57,540 --> 00:05:01,520
And that we write in this sort of mathy notation over here

94
00:05:01,520 --> 00:05:06,190
that says the function at the value 4 is the expected value

95
00:05:06,190 --> 00:05:08,370
of Y given X equals 4.

96
00:05:08,370 --> 00:05:11,980
And that expected value is just a fancy word for average.

97
00:05:11,980 --> 00:05:15,870
It&#39;s actually a conditional average given X equals 4.

98
00:05:15,870 --> 00:05:20,410
Since we can only deliver one value of the function at X

99
00:05:20,410 --> 00:05:25,370
equals 4, the average seems like a good value.

100
00:05:25,370 --> 00:05:28,690
And if we do that at each value of X, so at every single

101
00:05:28,690 --> 00:05:31,730
value of X, we deliver back the average of the Y&#39;s that

102
00:05:31,730 --> 00:05:37,180
have that value of X. Say, for example, at X equals 5, again,

103
00:05:37,180 --> 00:05:40,230
we want to have the average value in this little

104
00:05:40,230 --> 00:05:42,120
conditional slice here.

105
00:05:42,120 --> 00:05:45,360
That will trace out this little red curve that we have.

106
00:05:45,360 --> 00:05:47,340
And that&#39;s called the regression function.

107
00:05:47,340 --> 00:05:50,070
So the regression function gives you the conditional

108
00:05:50,070 --> 00:05:55,260
expectation of Y given X at each value of X. So that, in a

109
00:05:55,260 --> 00:06:00,840
sense, is the ideal function for a population in this case

110
00:06:00,840 --> 00:06:04,670
of Y and a single X.

111
00:06:04,670 --> 00:06:07,200
So let&#39;s talk more about this regression function.

112
00:06:07,200 --> 00:06:11,230
It&#39;s also defined for a vector X. So if X has got three

113
00:06:11,230 --> 00:06:14,390
components, for example, it&#39;s going to be the conditional

114
00:06:14,390 --> 00:06:18,860
expectation of Y given the three particular instances of

115
00:06:18,860 --> 00:06:24,980
the three components of X. So if you think about that, let&#39;s

116
00:06:24,980 --> 00:06:28,210
think of X as being two dimensional because we can

117
00:06:28,210 --> 00:06:29,780
think in three dimensions.

118
00:06:29,780 --> 00:06:35,910
So let&#39;s say X lies on the table, two dimensional X, and

119
00:06:35,910 --> 00:06:38,140
Y stands up vertically.

120
00:06:38,140 --> 00:06:40,590
So the idea is the same.

121
00:06:40,590 --> 00:06:44,540
We&#39;ve got a whole continuous cloud of Y&#39;s and X&#39;s.

122
00:06:44,540 --> 00:06:48,470
We go to a particular point X with two coordinates, X1 and

123
00:06:48,470 --> 00:06:50,820
X2, and we say, what&#39;s a good value for the

124
00:06:50,820 --> 00:06:52,870
function at that point?

125
00:06:52,870 --> 00:06:55,240
Well, we&#39;re just going to go up in the slice and average

126
00:06:55,240 --> 00:06:57,125
the Y&#39;s above that point.

127
00:06:57,125 --> 00:07:00,860
And we&#39;ll do that at all points in the plane.

128
00:07:00,860 --> 00:07:03,610
We say that&#39;s the ideal or optimal predictor of Y with

129
00:07:03,610 --> 00:07:06,590
regard for the function.

130
00:07:06,590 --> 00:07:10,460
And what that means is actually it&#39;s with regard to a

131
00:07:10,460 --> 00:07:11,230
loss function.

132
00:07:11,230 --> 00:07:13,840
What it means is that particular choice of the

133
00:07:13,840 --> 00:07:18,320
function f of X will minimize the sum of squared errors.

134
00:07:18,320 --> 00:07:23,030
Which we write in this fashion, again, expected value

135
00:07:23,030 --> 00:07:29,850
of Y minus g of X of all functions g at each point X.

136
00:07:29,850 --> 00:07:32,980
So it minimizes the average prediction errors.

137
00:07:36,650 --> 00:07:40,850
Now, at each point X, we&#39;re going to make mistakes because

138
00:07:40,850 --> 00:07:45,020
if we use this function to predict Y. Because there&#39;s

139
00:07:45,020 --> 00:07:47,020
lots of Y&#39;s at each point X. Right?

140
00:07:47,020 --> 00:07:50,480
And so the areas that we make, we call, in

141
00:07:50,480 --> 00:07:51,870
this case, them epsilons.

142
00:07:51,870 --> 00:07:54,900
And those are the irreducible error.

143
00:07:54,900 --> 00:07:58,270
You might know the ideal function f, but, of course, it

144
00:07:58,270 --> 00:08:00,900
doesn&#39;t make perfect predictions at each point X.

145
00:08:00,900 --> 00:08:02,670
So it has to make some errors.

146
00:08:02,670 --> 00:08:04,000
But, on average, it does well.

147
00:08:09,460 --> 00:08:13,760
For any estimate f hat of X. And that&#39;s what we tend to do.

148
00:08:13,760 --> 00:08:20,570
We tend to put these little hats on estimators to show

149
00:08:20,570 --> 00:08:24,570
that they&#39;ve been estimated from data.

150
00:08:24,570 --> 00:08:29,940
And so f hat of X is an estimate of f of X, we can

151
00:08:29,940 --> 00:08:36,059
expand the squared prediction error at X into two pieces.

152
00:08:36,058 --> 00:08:39,789
There&#39;s the irreducible piece which is just the variance of

153
00:08:39,789 --> 00:08:41,720
the errors.

154
00:08:41,720 --> 00:08:43,990
And there&#39;s the reducible piece which is the difference

155
00:08:43,990 --> 00:08:48,660
between our estimate, f hat of X, and the true

156
00:08:48,660 --> 00:08:51,830
function, f of X. OK.

157
00:08:51,830 --> 00:08:53,210
And that&#39;s a squared component.

158
00:08:53,210 --> 00:08:56,520
So this expected prediction error breaks up

159
00:08:56,520 --> 00:08:58,790
into these two pieces.

160
00:08:58,790 --> 00:09:00,410
So that&#39;s important to bear in mind.

161
00:09:00,410 --> 00:09:03,570
So if we want to improve our model, it&#39;s this first piece,

162
00:09:03,570 --> 00:09:06,520
the reducible piece that we can improve by maybe changing

163
00:09:06,520 --> 00:09:12,880
the way we estimate f of X. OK, so that&#39;s all nice.

164
00:09:12,880 --> 00:09:15,130
This is a kind of, up to now, has been somewhat of a

165
00:09:15,130 --> 00:09:16,900
theoretical exercise.

166
00:09:16,900 --> 00:09:20,300
Well, how do we estimate the function f?

167
00:09:20,300 --> 00:09:23,130
So the problem is we can&#39;t carry out this recipe of

168
00:09:23,130 --> 00:09:26,630
conditional expectation or conditional averaging exactly

169
00:09:26,630 --> 00:09:32,070
because at any given X in our data set, we might not have

170
00:09:32,070 --> 00:09:33,330
many points to average.

171
00:09:33,330 --> 00:09:36,160
We might not have any points to average.

172
00:09:36,160 --> 00:09:39,845
In the figure, we&#39;ve got a much smaller data set now.

173
00:09:39,845 --> 00:09:42,460
And we&#39;ve still got the point X equals 4.

174
00:09:42,460 --> 00:09:45,110
And if you look there, you&#39;ll see carefully that the solid

175
00:09:45,110 --> 00:09:47,520
point is one point up, I put on the plot,

176
00:09:47,520 --> 00:09:50,930
the solid green point.

177
00:09:50,930 --> 00:09:53,150
There&#39;s actually no data points whose X

178
00:09:53,150 --> 00:09:55,480
value is exactly 4.

179
00:09:55,480 --> 00:09:57,810
So how can we compute the conditional

180
00:09:57,810 --> 00:10:01,080
expectation or average?

181
00:10:01,080 --> 00:10:07,290
Well, what we can do is relax the idea of at the point X to

182
00:10:07,290 --> 00:10:10,210
at in a neighborhood of the point X. And so that&#39;s what

183
00:10:10,210 --> 00:10:12,690
the notation here refers to.

184
00:10:12,690 --> 00:10:16,920
N of x or script N of x is a neighborhood of points defined

185
00:10:16,920 --> 00:10:20,270
in some way around the target point which is

186
00:10:20,270 --> 00:10:22,750
this X equals 4 here.

187
00:10:22,750 --> 00:10:25,490
And it keeps the spirit of conditional expectation.

188
00:10:25,490 --> 00:10:28,660
It&#39;s close to the target point X. And if we make that

189
00:10:28,660 --> 00:10:31,220
neighborhood wide enough, we&#39;ll have enough points in

190
00:10:31,220 --> 00:10:32,680
the neighborhood to average.

191
00:10:32,680 --> 00:10:35,630
And we&#39;ll use their average to estimate the conditional

192
00:10:35,630 --> 00:10:37,710
expectation.

193
00:10:37,710 --> 00:10:40,630
So this is called nearest neighbors or local averaging.

194
00:10:40,630 --> 00:10:43,300
It&#39;s a very clever idea.

195
00:10:43,300 --> 00:10:44,290
It&#39;s not my idea.

196
00:10:44,290 --> 00:10:46,820
It has been invented long time ago.

197
00:10:46,820 --> 00:10:49,570
And, of course, you&#39;ll move this neighborhood, you&#39;ll

198
00:10:49,570 --> 00:10:52,390
slide this neighborhood along the x-axis.

199
00:10:52,390 --> 00:10:56,590
And as you compute the averages, as you slide in

200
00:10:56,590 --> 00:10:59,100
along, it&#39;ll trace out a curve.

201
00:10:59,100 --> 00:11:03,930
So that&#39;s actually a very good estimate of the function f.

202
00:11:03,930 --> 00:11:08,110
It&#39;s not going to be perfect because the little window, it

203
00:11:08,110 --> 00:11:09,560
has a certain width.

204
00:11:09,560 --> 00:11:13,910
And so as we can see, some points of the true f may be

205
00:11:13,910 --> 00:11:15,000
lower and some points higher.

206
00:11:15,000 --> 00:11:17,460
But on average, it does quite well.

207
00:11:17,460 --> 00:11:20,440
So we had a pretty powerful tool here for estimating this

208
00:11:20,440 --> 00:11:23,770
conditional expectation, just relax the definition, and

209
00:11:23,770 --> 00:11:27,040
compute the nearest neighbor average.

210
00:11:27,040 --> 00:11:29,060
And that gives us a fairly flexible way

211
00:11:29,060 --> 00:11:31,600
of footing a function.

212
00:11:31,600 --> 00:11:36,130
We&#39;ll see in the next section that this doesn&#39;t always work,

213
00:11:36,130 --> 00:11:38,490
especially as the dimensions get larger.

214
00:11:38,490 --> 00:11:40,370
And we&#39;ll have to have ways of dealing with it.

