0
00:00:00,000 --> 00:00:00,320
Hi.

1
00:00:00,320 --> 00:00:01,270
I&#39;m Trevor Hastie.

2
00:00:01,270 --> 00:00:02,922
And I&#39;m Rob Tibshirani.

3
00:00:02,922 --> 00:00:03,746
Cut.

4
00:00:03,746 --> 00:00:04,570
I&#39;m not--

5
00:00:04,570 --> 00:00:06,550
And then you say, welcome to the course [INAUDIBLE].

6
00:00:06,550 --> 00:00:07,510
Hi, I&#39;m Trevor Hastie.

7
00:00:07,510 --> 00:00:08,170
And I&#39;m Rob--

8
00:00:08,170 --> 00:00:08,642
Cut.

9
00:00:08,642 --> 00:00:09,586
[LAUGHING]

10
00:00:09,586 --> 00:00:10,790
Hi, I&#39;m Trevor.

11
00:00:10,790 --> 00:00:11,910
No, I&#39;m Rob Tibshirani.

12
00:00:11,910 --> 00:00:13,710
And I&#39;m Trevor Hastie.

13
00:00:13,710 --> 00:00:16,355
And welcome to our course on statistical learning.

14
00:00:16,355 --> 00:00:18,700
It&#39;s the first online course we&#39;ve ever given.

15
00:00:18,700 --> 00:00:20,530
And we&#39;re excited to tell you about it.

16
00:00:20,530 --> 00:00:22,430
And a little nervous, as you can hear.

17
00:00:22,430 --> 00:00:26,220
So by way of background, what is statistical learning?

18
00:00:26,220 --> 00:00:27,430
Trevor and I are both statisticians.

19
00:00:27,430 --> 00:00:28,660
We were actually graduate students here at

20
00:00:28,660 --> 00:00:30,010
Stanford in the &#39;80s.

21
00:00:30,010 --> 00:00:31,340
We&#39;ve known each other for about 30 years.

22
00:00:31,340 --> 00:00:32,330
Oh, my goodness.

23
00:00:32,330 --> 00:00:35,770
And back then, well, we did applied statistics, like a lot

24
00:00:35,770 --> 00:00:36,680
of statisticians did.

25
00:00:36,680 --> 00:00:40,610
Statistics has been around since about 1900 or before.

26
00:00:40,610 --> 00:00:43,680
But in the 1980s, people in computer science developed the

27
00:00:43,680 --> 00:00:45,350
field of machine learning.

28
00:00:45,350 --> 00:00:47,520
Especially, neural networks became a very hot topic.

29
00:00:47,520 --> 00:00:49,095
I was at University of Toronto, and

30
00:00:49,095 --> 00:00:50,440
Trevor was at Bell Labs.

31
00:00:50,440 --> 00:00:53,370
And one of the first neural networks was developed at Bell

32
00:00:53,370 --> 00:00:56,860
Labs to solve the ZIP code recognition problem, which

33
00:00:56,860 --> 00:00:59,480
we&#39;ll show you a little bit about in a few slides.

34
00:00:59,480 --> 00:01:02,320
So around that time, Trevor and I and then some

35
00:01:02,320 --> 00:01:05,050
colleagues, Jerry Friedman, Brad Efron--

36
00:01:05,050 --> 00:01:06,340
[? Neil Bryman ?]

37
00:01:06,340 --> 00:01:09,410
And actually, you&#39;ll hear from Jerry and Brad,

38
00:01:09,410 --> 00:01:10,550
both in this course.

39
00:01:10,550 --> 00:01:12,510
We have some interviews with them.

40
00:01:12,510 --> 00:01:14,820
About that time, we started to work on the area of machine

41
00:01:14,820 --> 00:01:18,110
learning and sort of developed our own view of it, which is

42
00:01:18,110 --> 00:01:19,050
now called statistical learning.

43
00:01:19,050 --> 00:01:22,590
So along with colleagues here at Stanford and other places,

44
00:01:22,590 --> 00:01:25,310
we developed this field of statistical learning.

45
00:01:25,310 --> 00:01:26,660
So in this course, we&#39;ll talk to you about some of the

46
00:01:26,660 --> 00:01:29,100
developments in this area and give lots of examples.

47
00:01:29,100 --> 00:01:34,290
So let&#39;s start with our first example, which is a computer

48
00:01:34,290 --> 00:01:38,130
program playing Jeopardy called Watson that IBM built.

49
00:01:38,130 --> 00:01:43,250
And it beat the players in a three game match.

50
00:01:43,250 --> 00:01:46,160
And the people at IBM who developed the system said it

51
00:01:46,160 --> 00:01:47,950
was really a triumph of machine learning.

52
00:01:47,950 --> 00:01:50,350
There were a lot of very smart technology, both hardware, but

53
00:01:50,350 --> 00:01:52,230
also the software and the algorithms were based on

54
00:01:52,230 --> 00:01:53,210
machine learning.

55
00:01:53,210 --> 00:01:56,400
So this was a watershed moment, I think, for the area

56
00:01:56,400 --> 00:01:58,340
of artificial intelligence and machine learning.

57
00:02:01,290 --> 00:02:05,710
Google is a big user of data and a big analyzer of data.

58
00:02:05,710 --> 00:02:09,680
And here&#39;s a quote that came in the New York Times in 2009

59
00:02:09,680 --> 00:02:12,850
from Hal Varian, who&#39;s the Chief Economist at Google.

60
00:02:12,850 --> 00:02:14,820
You can see the quote there. &quot;I keep saying that the sexy

61
00:02:14,820 --> 00:02:17,320
job in the next 10 years will be statisticians.&quot;

62
00:02:17,320 --> 00:02:19,500
And indeed, there&#39;s a picture of Carrie Grimes, who was a

63
00:02:19,500 --> 00:02:21,820
graduate from Stanford Statistics.

64
00:02:21,820 --> 00:02:24,840
She was one of the first statisticians hired at Google.

65
00:02:24,840 --> 00:02:28,130
Now Google has many statisticians.

66
00:02:28,130 --> 00:02:30,020
Our next example-- this is a picture of Nate

67
00:02:30,020 --> 00:02:30,830
Silver on the right.

68
00:02:30,830 --> 00:02:33,210
Nate has a masters in economics, but he calls

69
00:02:33,210 --> 00:02:34,800
himself a statistician.

70
00:02:34,800 --> 00:02:37,260
And he writes-- at least he did write a blog called

71
00:02:37,260 --> 00:02:39,270
FiveThirtyEight for the New York Times.

72
00:02:39,270 --> 00:02:44,280
And in that blog, he predicted the outcome of the 2012

73
00:02:44,280 --> 00:02:47,820
Presidential and Senate elections very well.

74
00:02:47,820 --> 00:02:50,620
As a matter of fact, he got all the Senate races right.

75
00:02:50,620 --> 00:02:54,860
And the Presidential election, he predicted very accurately

76
00:02:54,860 --> 00:02:58,580
using statistics, using carefully sampled data from

77
00:02:58,580 --> 00:03:00,600
various places, some careful analysis.

78
00:03:00,600 --> 00:03:03,160
He did an extremely accurate job of predicting the election

79
00:03:03,160 --> 00:03:05,990
when a lot of news outlets weren&#39;t sure

80
00:03:05,990 --> 00:03:06,780
who&#39;s going to win.

81
00:03:06,780 --> 00:03:08,465
Pretty nerdy looking guy, isn&#39;t he, Rob?

82
00:03:08,465 --> 00:03:09,080
Yes.

83
00:03:09,080 --> 00:03:09,920
But he&#39;s really famous.

84
00:03:09,920 --> 00:03:10,280
And--

85
00:03:10,280 --> 00:03:12,080
He&#39;s like a rock star these days.

86
00:03:12,080 --> 00:03:12,420
Yes.

87
00:03:12,420 --> 00:03:13,320
We joke about--

88
00:03:13,320 --> 00:03:15,610
when you&#39;re a statistician, when you go to a party and

89
00:03:15,610 --> 00:03:17,060
someone says, what do you do?

90
00:03:17,060 --> 00:03:18,090
When you say, I&#39;m a statistician,

91
00:03:18,090 --> 00:03:19,900
they run for the door.

92
00:03:19,900 --> 00:03:22,160
But nowadays, we can say, well, we do machine learning.

93
00:03:22,160 --> 00:03:23,750
And well, they still run for the door.

94
00:03:23,750 --> 00:03:25,690
But they take a little longer to get there.

95
00:03:25,690 --> 00:03:27,935
In fact, we now call ourselves data scientists.

96
00:03:27,935 --> 00:03:30,750
It&#39;s a trendier word.

97
00:03:30,750 --> 00:03:33,350
So we&#39;re going to run through a number of statistical

98
00:03:33,350 --> 00:03:34,750
learning problems.

99
00:03:34,750 --> 00:03:37,240
You can see there&#39;s a bunch of examples on this page.

100
00:03:37,240 --> 00:03:39,040
And we&#39;ll go through them one by one, just to give you a

101
00:03:39,040 --> 00:03:42,230
flavor of what sort of problems we&#39;re going to be

102
00:03:42,230 --> 00:03:43,540
thinking about.

103
00:03:43,540 --> 00:03:46,560
So the first data set we&#39;re going to look at is on

104
00:03:46,560 --> 00:03:48,330
prostate cancer.

105
00:03:48,330 --> 00:03:54,100
This is a relatively small data set, 97 men, sampled from

106
00:03:54,100 --> 00:03:58,090
97 men with prostate cancer actually by a Stanford

107
00:03:58,090 --> 00:04:01,340
physician, Doctor Stamey, in the late &#39;80s.

108
00:04:01,340 --> 00:04:05,920
And what we have is the PSA measurement for each subject,

109
00:04:05,920 --> 00:04:08,850
along with a number of clinical and blood

110
00:04:08,850 --> 00:04:10,470
measurements from the patients.

111
00:04:10,470 --> 00:04:13,640
Some measurements are on the cancer itself and some

112
00:04:13,640 --> 00:04:17,390
measurements from the blood, the measurements to do with

113
00:04:17,390 --> 00:04:21,560
cancer size and the severity of the cancer.

114
00:04:21,560 --> 00:04:24,140
And this is a scatter plot matrix, which

115
00:04:24,140 --> 00:04:26,270
actually shows the data.

116
00:04:26,270 --> 00:04:28,410
And you see on the diagonal is the name

117
00:04:28,410 --> 00:04:29,830
of each of the variables.

118
00:04:29,830 --> 00:04:31,830
And each little plot is a pair of variable.

119
00:04:31,830 --> 00:04:35,090
So you get in one picture, if you&#39;ve got a relatively small

120
00:04:35,090 --> 00:04:37,990
number of variables, you can see all the data at once in a

121
00:04:37,990 --> 00:04:38,730
picture like this.

122
00:04:38,730 --> 00:04:41,700
And you can see the nature of the data, what variables are

123
00:04:41,700 --> 00:04:43,320
correlated and so on.

124
00:04:43,320 --> 00:04:46,960
And so this is a good way of getting a view of your data.

125
00:04:46,960 --> 00:04:53,110
And in this particular case, the goal was to try and

126
00:04:53,110 --> 00:04:55,810
predict the PSA from the other measurements.

127
00:04:55,810 --> 00:04:56,980
So it&#39;s along the top.

128
00:04:56,980 --> 00:04:59,490
And you can see there&#39;s some correlations between these

129
00:04:59,490 --> 00:05:01,450
measurements.

130
00:05:01,450 --> 00:05:05,260
This is actually another view of these data which looks

131
00:05:05,260 --> 00:05:09,730
rather similar except in one instance over here, which is--

132
00:05:09,730 --> 00:05:10,770
this is the log weight.

133
00:05:10,770 --> 00:05:12,890
These variables are on the log scale.

134
00:05:12,890 --> 00:05:14,890
And this is log weight.

135
00:05:14,890 --> 00:05:17,050
And you notice there&#39;s a point over here.

136
00:05:17,050 --> 00:05:19,990
It looks like somewhat of an outlier.

137
00:05:19,990 --> 00:05:22,100
Well, it turns out on the log scale, it looks

138
00:05:22,100 --> 00:05:23,210
a bit like an outlier.

139
00:05:23,210 --> 00:05:27,100
But when you look on the normal scale, it&#39;s enormous.

140
00:05:27,100 --> 00:05:28,980
And basically, that was a typo.

141
00:05:28,980 --> 00:05:30,970
And that would say--

142
00:05:30,970 --> 00:05:34,610
if that was a real measurement, it would say that

143
00:05:34,610 --> 00:05:41,190
this particular patient would have a 449 gram prostate.

144
00:05:41,190 --> 00:05:45,200
Well, we got a message from a retired urologist, Doctor

145
00:05:45,200 --> 00:05:47,140
Steven Link, who pointed this out to us.

146
00:05:47,140 --> 00:05:50,850
And so we corrected an earlier published version of this

147
00:05:50,850 --> 00:05:52,000
scatter plot.

148
00:05:52,000 --> 00:05:52,850
Which is a good thing to remember.

149
00:05:52,850 --> 00:05:55,290
The first thing to do when you get a set of data for analysis

150
00:05:55,290 --> 00:05:56,910
is not to run it through a fancy algorithm.

151
00:05:56,910 --> 00:05:58,610
Make some graphs, some plots.

152
00:05:58,610 --> 00:05:59,740
Look at the data.

153
00:05:59,740 --> 00:06:01,550
I think in the old days before computers, people did that

154
00:06:01,550 --> 00:06:03,460
much more because it was easier.

155
00:06:03,460 --> 00:06:04,820
I mean, you do it by hand.

156
00:06:04,820 --> 00:06:06,480
And your analysis took many, many hours.

157
00:06:06,480 --> 00:06:08,290
So people would look first at the data much more.

158
00:06:08,290 --> 00:06:11,250
And we need to remember that, that even with big data, you

159
00:06:11,250 --> 00:06:15,020
should look at it first before you jump in with an analysis.

160
00:06:15,020 --> 00:06:20,470
So the next example is phonemes for two vowel sounds.

161
00:06:20,470 --> 00:06:24,960
And this is looking at-- this graph has the log periodograms

162
00:06:24,960 --> 00:06:26,940
for two different phonemes, the power at different

163
00:06:26,940 --> 00:06:31,670
frequencies for two different phonemes, AA and AO.

164
00:06:31,670 --> 00:06:32,645
How do you pronounce those, Trevor?

165
00:06:32,645 --> 00:06:35,742
AA is odd, and AO is ought.

166
00:06:35,742 --> 00:06:37,320
So as you can tell, Trevor talks funny.

167
00:06:37,320 --> 00:06:39,140
But hopefully, during the course, you&#39;ll be able to--

168
00:06:39,140 --> 00:06:40,476
Well, how can you say it, then?

169
00:06:40,476 --> 00:06:42,600
Odd and ought?

170
00:06:42,600 --> 00:06:43,560
OK.

171
00:06:43,560 --> 00:06:47,640
So you see the log periodograms at various

172
00:06:47,640 --> 00:06:50,990
frequencies of these two vowel sounds as spoken by different

173
00:06:50,990 --> 00:06:53,390
people, the orange and the green.

174
00:06:53,390 --> 00:06:57,390
And the goal here is to try to classify the two vowel sounds

175
00:06:57,390 --> 00:06:59,320
based on the power at different frequencies.

176
00:06:59,320 --> 00:07:04,200
So on the bottom, we see a logit model&#39;s been fit to the

177
00:07:04,200 --> 00:07:07,870
data, looking at trying to classify the two classes from

178
00:07:07,870 --> 00:07:11,120
each other based on the power of different frequencies.

179
00:07:11,120 --> 00:07:13,420
The logit model&#39;s from logistic regression, which is

180
00:07:13,420 --> 00:07:16,805
used to classify into one of the two vowel sounds, based on

181
00:07:16,805 --> 00:07:18,000
the log periodogram.

182
00:07:18,000 --> 00:07:20,010
And we&#39;ll cover it in detail in the course.

183
00:07:20,010 --> 00:07:22,730
And the coefficients, the estimated coefficients from

184
00:07:22,730 --> 00:07:27,940
the logistical model are in the gray profiles here in the

185
00:07:27,940 --> 00:07:28,570
bottom plot.

186
00:07:28,570 --> 00:07:32,200
And you can see, they&#39;re very non-smooth.

187
00:07:32,200 --> 00:07:34,310
You&#39;d be hard pressed to tell where the important

188
00:07:34,310 --> 00:07:35,590
frequencies were.

189
00:07:35,590 --> 00:07:37,510
But when you apply a kind of smoothing, which we&#39;ll also

190
00:07:37,510 --> 00:07:38,790
discuss in the course--

191
00:07:38,790 --> 00:07:41,460
we use the fact that the nearby frequencies should be

192
00:07:41,460 --> 00:07:46,140
similar, which the gray did not exploit.

193
00:07:46,140 --> 00:07:47,520
We get the red curve.

194
00:07:47,520 --> 00:07:49,150
And the red curve shows you pretty clearly that the

195
00:07:49,150 --> 00:07:50,840
important frequencies--

196
00:07:50,840 --> 00:07:53,820
looks like the one vowel sound&#39;s got more power around

197
00:07:53,820 --> 00:07:56,780
25, and the other vowel sound has more power

198
00:07:56,780 --> 00:07:58,030
around just before 50.

199
00:08:01,970 --> 00:08:03,910
Predict whether someone will have a heart attack on the

200
00:08:03,910 --> 00:08:07,430
basis of demographic, diet, and clinical measurements.

201
00:08:07,430 --> 00:08:10,450
So these are some data on actually

202
00:08:10,450 --> 00:08:12,120
men from South Africa.

203
00:08:12,120 --> 00:08:15,090
The red ones are those that had heart disease, and the

204
00:08:15,090 --> 00:08:16,560
blue points are those that didn&#39;t.

205
00:08:16,560 --> 00:08:18,430
It&#39;s a case controlled sample.

206
00:08:18,430 --> 00:08:21,860
So all the heart attack cases were taken as cases, and a

207
00:08:21,860 --> 00:08:23,880
sample of the controls remained.

208
00:08:23,880 --> 00:08:25,530
And the idea was to understand the risk

209
00:08:25,530 --> 00:08:28,130
factors in heart disease.

210
00:08:28,130 --> 00:08:30,110
Now when you have a binary response like this, you can

211
00:08:30,110 --> 00:08:32,630
color the scatter plot matrix so you can see the points,

212
00:08:32,630 --> 00:08:34,870
which is rather handy.

213
00:08:34,870 --> 00:08:38,110
And these data come from a region of South Africa where

214
00:08:38,110 --> 00:08:40,860
the risk of heart disease is very high.

215
00:08:40,860 --> 00:08:44,280
It&#39;s over 5% for this age group.

216
00:08:44,280 --> 00:08:47,690
The people, especially men around there, eat lots of--

217
00:08:47,690 --> 00:08:49,460
these were men.

218
00:08:49,460 --> 00:08:50,410
They eat lots of meat.

219
00:08:50,410 --> 00:08:53,380
They have meat for all three meals.

220
00:08:53,380 --> 00:08:57,080
And in fact, meat&#39;s so prevalent, chickens regard it

221
00:08:57,080 --> 00:08:58,490
as a vegetable.

222
00:08:58,490 --> 00:08:59,230
Poor chicken.

223
00:08:59,230 --> 00:09:00,720
Rob loves that joke.

224
00:09:00,720 --> 00:09:02,010
I used to love that joke.

225
00:09:02,010 --> 00:09:02,360
OK.

226
00:09:02,360 --> 00:09:05,610
So again, you can see there&#39;s correlations in these data.

227
00:09:05,610 --> 00:09:10,810
And the goal here is to fit a model that jointly involves

228
00:09:10,810 --> 00:09:13,620
all these different risk factors in coming up with a

229
00:09:13,620 --> 00:09:19,290
risk model for heart disease, which in this case, is as I

230
00:09:19,290 --> 00:09:22,690
said, colored in red.

231
00:09:22,690 --> 00:09:25,730
Our next example is email spam detection.

232
00:09:25,730 --> 00:09:29,890
As everyone uses email, and spam is definitely a problem,

233
00:09:29,890 --> 00:09:32,720
so spam filters are a very important application of

234
00:09:32,720 --> 00:09:34,390
statistical machine learning.

235
00:09:34,390 --> 00:09:37,325
The data on this table actually--

236
00:09:37,325 --> 00:09:40,040
I think it&#39;s from maybe the late &#39;90s.

237
00:09:40,040 --> 00:09:40,640
Is that right?

238
00:09:40,640 --> 00:09:41,260
Before-- before--

239
00:09:41,260 --> 00:09:42,030
yeah.

240
00:09:42,030 --> 00:09:42,820
Late &#39;90s, exactly.

241
00:09:42,820 --> 00:09:44,200
It&#39;s from Hewlett Packard.

242
00:09:44,200 --> 00:09:46,340
So this is a person named George who

243
00:09:46,340 --> 00:09:47,230
worked at Hewlett Packard.

244
00:09:47,230 --> 00:09:50,450
So this was early in the days of email, where as well spam

245
00:09:50,450 --> 00:09:52,330
was also not very sophisticated.

246
00:09:52,330 --> 00:09:55,670
So what we have here is data from over 4,000 emails sent to

247
00:09:55,670 --> 00:09:58,040
an individual named George at HP Labs.

248
00:09:58,040 --> 00:09:59,770
Each one&#39;s been hand labeled as either being

249
00:09:59,770 --> 00:10:01,300
spam or good email.

250
00:10:01,300 --> 00:10:03,030
And the goal here is to try to predict--

251
00:10:03,030 --> 00:10:05,310
Actually, they call good email ham these days, Rob.

252
00:10:05,310 --> 00:10:06,780
OK.

253
00:10:06,780 --> 00:10:11,660
So a goal was to try to classify spam from ham based

254
00:10:11,660 --> 00:10:15,580
on the frequencies of words in the email.

255
00:10:15,580 --> 00:10:18,300
So here we just have a summary table of some of the more

256
00:10:18,300 --> 00:10:19,630
important features.

257
00:10:19,630 --> 00:10:24,680
So it&#39;s based on words and characters in the email.

258
00:10:24,680 --> 00:10:27,430
So for example, this is saying that if this email had George

259
00:10:27,430 --> 00:10:30,350
in it, it was more likely to be good email than spam.

260
00:10:30,350 --> 00:10:34,550
Back then, if your name&#39;s George, and you saw George in

261
00:10:34,550 --> 00:10:36,060
your email, it was more likely to be good email.

262
00:10:36,060 --> 00:10:39,360
Nowadays, of course, spam is much more sophisticated.

263
00:10:39,360 --> 00:10:39,990
They know your name.

264
00:10:39,990 --> 00:10:41,040
They know a lot about your life.

265
00:10:41,040 --> 00:10:44,910
And the fact that your name&#39;s in it actually may be a

266
00:10:44,910 --> 00:10:46,370
smaller chance it&#39;s actually good email.

267
00:10:46,370 --> 00:10:50,540
But back then, spammers were much less sophisticated.

268
00:10:50,540 --> 00:10:53,770
So for example, if your name was in it, it more chance to

269
00:10:53,770 --> 00:10:54,430
be hot email.

270
00:10:54,430 --> 00:10:56,290
What&#39;s with that remove word, Rob?

271
00:10:56,290 --> 00:10:56,980
Remove.

272
00:10:56,980 --> 00:10:57,360
OK.

273
00:10:57,360 --> 00:11:00,360
So I guess it probably said something like, don&#39;t remove.

274
00:11:00,360 --> 00:11:01,696
Is that right?

275
00:11:01,696 --> 00:11:04,280
I think it says, if you want to be removed

276
00:11:04,280 --> 00:11:05,310
from this list, click.

277
00:11:05,310 --> 00:11:05,700
I see.

278
00:11:05,700 --> 00:11:07,630
That&#39;s usually a spam.

279
00:11:07,630 --> 00:11:10,220
So the goal was that--

280
00:11:10,220 --> 00:11:13,480
and we&#39;ll talk about this example in detail-- to use the

281
00:11:13,480 --> 00:11:15,150
57 features--

282
00:11:15,150 --> 00:11:17,690
and these are seven of those features--

283
00:11:17,690 --> 00:11:20,610
as a classifier together to try to predict whether an

284
00:11:20,610 --> 00:11:22,360
email is spam or ham.

285
00:11:26,350 --> 00:11:28,630
Identify the numbers in a hand written ZIP code.

286
00:11:28,630 --> 00:11:30,800
This is what we were alluding to earlier.

287
00:11:30,800 --> 00:11:35,930
Here&#39;s some handwritten digits taken from envelopes.

288
00:11:35,930 --> 00:11:40,530
And the goal is to, based on an image of any of these

289
00:11:40,530 --> 00:11:43,640
digits, say what the digit is, to classify into

290
00:11:43,640 --> 00:11:45,640
the 10 digit classes.

291
00:11:45,640 --> 00:11:49,470
Well to humans, this looks like a pretty easy task.

292
00:11:49,470 --> 00:11:51,360
We&#39;re pretty good at pattern recognition.

293
00:11:51,360 --> 00:11:52,970
It turns out it&#39;s a notoriously

294
00:11:52,970 --> 00:11:54,840
difficult task for computers.

295
00:11:54,840 --> 00:11:57,260
They&#39;re getting better and better all the time.

296
00:11:57,260 --> 00:12:02,950
So this was one of the first learning tasks that was used

297
00:12:02,950 --> 00:12:05,500
to develop neural networks.

298
00:12:05,500 --> 00:12:06,840
Neural networks were first brought to

299
00:12:06,840 --> 00:12:08,480
bear on this problem.

300
00:12:08,480 --> 00:12:13,470
And we thought this should be an easy problem to crack.

301
00:12:13,470 --> 00:12:15,570
It turns out it&#39;s really difficult.

302
00:12:15,570 --> 00:12:17,320
Actually, I remember the first time, Trevor, that we worked

303
00:12:17,320 --> 00:12:19,180
on a machine learning problem, it was this problem.

304
00:12:19,180 --> 00:12:20,300
And you were working at Bell Labs.

305
00:12:20,300 --> 00:12:21,740
I visited Bell Labs.

306
00:12:21,740 --> 00:12:22,890
And you had just gotten this data.

307
00:12:22,890 --> 00:12:24,860
And you said, these people in artificial intelligence are

308
00:12:24,860 --> 00:12:25,570
working on this.

309
00:12:25,570 --> 00:12:27,790
And we thought, oh, let&#39;s try some statistical methods.

310
00:12:27,790 --> 00:12:29,840
And we tried discriminant analysis, right?

311
00:12:29,840 --> 00:12:30,410
That&#39;s right.

312
00:12:30,410 --> 00:12:31,370
And we got an error rate of about

313
00:12:31,370 --> 00:12:32,470
eight and a half percent.

314
00:12:32,470 --> 00:12:33,863
And the best error rate--

315
00:12:33,863 --> 00:12:34,950
In about 20 minutes or so.

316
00:12:34,950 --> 00:12:35,070
Right.

317
00:12:35,070 --> 00:12:37,360
And the best error rate anyone else had was about 4% or 5% at

318
00:12:37,360 --> 00:12:37,760
that point.

319
00:12:37,760 --> 00:12:39,490
We thought, oh, this is going to be easy.

320
00:12:39,490 --> 00:12:42,926
We&#39;re already at 8% in 10 or 15 minutes.

321
00:12:42,926 --> 00:12:44,355
Six months later--

322
00:12:44,355 --> 00:12:46,350
Six months later, we were maybe at the same place.

323
00:12:46,350 --> 00:12:47,490
So we realize actually--

324
00:12:47,490 --> 00:12:49,870
as often is the case, you can get some of the

325
00:12:49,870 --> 00:12:50,750
signal pretty quickly.

326
00:12:50,750 --> 00:12:54,150
But getting down to a very good error rate, in this case,

327
00:12:54,150 --> 00:12:57,440
trying to classify some of the harder to classify things,

328
00:12:57,440 --> 00:12:59,430
like maybe this four--

329
00:12:59,430 --> 00:13:01,070
or actually, most of these are pretty easy.

330
00:13:01,070 --> 00:13:03,520
But if you look at the database, some of them are

331
00:13:03,520 --> 00:13:06,050
very hard, so hard that the human eye can&#39;t really tell

332
00:13:06,050 --> 00:13:07,820
what they are or has difficulty.

333
00:13:07,820 --> 00:13:10,810
And those are the ones that machine learning algorithms

334
00:13:10,810 --> 00:13:12,245
area really challenged by.

335
00:13:12,245 --> 00:13:13,395
Anyway, it&#39;s a lovely problem.

336
00:13:13,395 --> 00:13:16,970
And it&#39;s fascinated machine learners and statisticians for

337
00:13:16,970 --> 00:13:18,850
a long time.

338
00:13:18,850 --> 00:13:22,360
So the next example comes from medicine, classifying a tissue

339
00:13:22,360 --> 00:13:25,380
sample into one of several cancer classes based on the

340
00:13:25,380 --> 00:13:27,020
gene expression profile.

341
00:13:27,020 --> 00:13:29,650
So Trevor and I both work in a medical school part time here

342
00:13:29,650 --> 00:13:30,760
at Stanford.

343
00:13:30,760 --> 00:13:33,550
And a lot of what we do and others do is to try to use

344
00:13:33,550 --> 00:13:38,100
machine learning, statistical learning, big data analysis to

345
00:13:38,100 --> 00:13:40,970
learn about data in cancer and other diseases.

346
00:13:40,970 --> 00:13:42,250
So this is an example of that.

347
00:13:42,250 --> 00:13:43,790
This is data in breast cancer.

348
00:13:43,790 --> 00:13:45,240
It&#39;s called gene expression data.

349
00:13:45,240 --> 00:13:48,190
So this has been collected from gene chips.

350
00:13:48,190 --> 00:13:51,590
And what we see here on the left is a matrix of data.

351
00:13:51,590 --> 00:13:53,130
Each row is a gene.

352
00:13:53,130 --> 00:13:56,200
And there&#39;s about 8,000 genes here I think.

353
00:13:56,200 --> 00:13:57,335
And each column is a patient.

354
00:13:57,335 --> 00:13:59,140
And this is called a heat map.

355
00:13:59,140 --> 00:14:02,105
So what this heat map is representing is low and high

356
00:14:02,105 --> 00:14:05,130
gene expression for a given patient for a given gene, so

357
00:14:05,130 --> 00:14:07,950
green being low and red meaning high.

358
00:14:07,950 --> 00:14:09,660
And gene expression means the gene is working.

359
00:14:09,660 --> 00:14:12,200
So if a gene is expressing, it&#39;s working hard in the cell.

360
00:14:12,200 --> 00:14:13,865
If it&#39;s not expressing, it quiet.

361
00:14:13,865 --> 00:14:15,300
It&#39;s silent.

362
00:14:15,300 --> 00:14:18,460
And the goal is to try to figure out which genes--

363
00:14:18,460 --> 00:14:20,670
well, try to figure out the pattern of gene expression.

364
00:14:20,670 --> 00:14:21,380
These are patients.

365
00:14:21,380 --> 00:14:23,630
These are women with breast cancer.

366
00:14:23,630 --> 00:14:25,400
Trying to figure out the common patterns of gene

367
00:14:25,400 --> 00:14:27,710
expression for women with breast cancer and seeing where

368
00:14:27,710 --> 00:14:30,140
there&#39;s subcategories of breast cancer showing

369
00:14:30,140 --> 00:14:31,710
different gene expression.

370
00:14:31,710 --> 00:14:34,200
So what we see here is a heat map of the full data.

371
00:14:34,200 --> 00:14:38,200
88 women in the columns and about 8,000 genes in the rows.

372
00:14:38,200 --> 00:14:41,080
And hierarchical clustering, which we&#39;ll discuss in the

373
00:14:41,080 --> 00:14:43,440
last part of this course, is being applied to the columns.

374
00:14:43,440 --> 00:14:45,720
And you see the clustering tree at the top here, which

375
00:14:45,720 --> 00:14:49,260
has been expanded for your view at the top.

376
00:14:49,260 --> 00:14:51,850
And hierarchical clustering is being used to divide these

377
00:14:51,850 --> 00:14:55,050
women into roughly one, two, three, four, five, six

378
00:14:55,050 --> 00:14:57,735
subgroups based on the gene expression.

379
00:14:57,735 --> 00:15:00,530
They&#39;re very effective, especially with these colors.

380
00:15:00,530 --> 00:15:02,710
You can just see these clusters standing out.

381
00:15:02,710 --> 00:15:03,075
Yeah.

382
00:15:03,075 --> 00:15:04,990
Hierarchical clustering and heat maps have actually been a

383
00:15:04,990 --> 00:15:07,630
very important contribution for genomics, which this is an

384
00:15:07,630 --> 00:15:10,680
example of, simply because they enable you to see and to

385
00:15:10,680 --> 00:15:13,990
organize the full set of data on just a single picture.

386
00:15:13,990 --> 00:15:16,880
In the bottom right here, we&#39;ve drilled down to look

387
00:15:16,880 --> 00:15:18,840
more at the gene expression.

388
00:15:18,840 --> 00:15:21,950
For example, this subgroup here, these red patients, seem

389
00:15:21,950 --> 00:15:25,130
to be high largely in these genes and

390
00:15:25,130 --> 00:15:26,980
maybe in these genes.

391
00:15:26,980 --> 00:15:29,300
So we&#39;ll talk about this example in detail later on in

392
00:15:29,300 --> 00:15:30,550
the course.

393
00:15:33,360 --> 00:15:36,230
Establish the relationship between salary and demographic

394
00:15:36,230 --> 00:15:39,320
variables in population survey data.

395
00:15:39,320 --> 00:15:41,950
So here&#39;s some survey data.

396
00:15:41,950 --> 00:15:46,190
We see income from the central Atlantic region

397
00:15:46,190 --> 00:15:48,910
of the USA in 2009.

398
00:15:48,910 --> 00:15:51,720
And you see what you might expect to see.

399
00:15:51,720 --> 00:15:55,250
As a function of age, income initially goes up, then levels

400
00:15:55,250 --> 00:15:59,050
off, and then finally goes down as people get older.

401
00:15:59,050 --> 00:16:02,490
Incomes gradually increase with year as the cost of

402
00:16:02,490 --> 00:16:03,930
living increases.

403
00:16:03,930 --> 00:16:07,350
And incomes change with education level.

404
00:16:07,350 --> 00:16:10,080
That&#39;s the right hand plot Those are box plots.

405
00:16:10,080 --> 00:16:13,580
And so here we see three of the

406
00:16:13,580 --> 00:16:15,320
variables that affect income.

407
00:16:15,320 --> 00:16:18,510
And again, the goal is-- we&#39;ll use regression models to try

408
00:16:18,510 --> 00:16:20,690
and understand the roles of these variables together and

409
00:16:20,690 --> 00:16:25,510
see if there&#39;s interactions and so on.

410
00:16:25,510 --> 00:16:28,750
And the last example is Landsat images

411
00:16:28,750 --> 00:16:32,080
of land use in Australia.

412
00:16:32,080 --> 00:16:33,890
So this is a rural area of Australia.

413
00:16:33,890 --> 00:16:35,470
Those are harsh colors, Rob.

414
00:16:35,470 --> 00:16:36,940
Did you choose those colors.

415
00:16:36,940 --> 00:16:37,990
You probably did, Trevor.

416
00:16:37,990 --> 00:16:39,160
You&#39;re the color [INAUDIBLE].

417
00:16:39,160 --> 00:16:41,720
This is before--

418
00:16:41,720 --> 00:16:44,010
When did that happen?

419
00:16:44,010 --> 00:16:46,270
I didn&#39;t see the news memo.

420
00:16:46,270 --> 00:16:46,670
OK.

421
00:16:46,670 --> 00:16:51,140
So these are from Landsat images.

422
00:16:51,140 --> 00:16:53,950
So let&#39;s start here in this panel.

423
00:16:53,950 --> 00:16:58,000
So this is again a rural area of Australia where the land

424
00:16:58,000 --> 00:17:01,320
use has been labeled, I think, actually by graduate students

425
00:17:01,320 --> 00:17:05,465
or researchers into one of one, two,

426
00:17:05,464 --> 00:17:08,050
three, four, five, six--

427
00:17:08,050 --> 00:17:11,363
They don&#39;t have to pay the graduate students as much

428
00:17:11,363 --> 00:17:12,589
there as we do.

429
00:17:12,589 --> 00:17:15,160
So they&#39;ve been labeled in these columns indicating the

430
00:17:15,160 --> 00:17:15,710
different labels.

431
00:17:15,710 --> 00:17:16,940
These are the true labels.

432
00:17:16,940 --> 00:17:20,099
And the goal is to try to predict these true labels from

433
00:17:20,098 --> 00:17:24,790
the spectral bands at four frequencies taken from a

434
00:17:24,790 --> 00:17:25,430
satellite image.

435
00:17:25,430 --> 00:17:29,760
And so here&#39;s the power at the different frequencies in four

436
00:17:29,760 --> 00:17:31,240
spectral bands.

437
00:17:31,240 --> 00:17:33,420
So we have features which are-- now they&#39;re pretty

438
00:17:33,420 --> 00:17:36,570
complicated because we have features, spatial features,

439
00:17:36,570 --> 00:17:37,510
four layers of them.

440
00:17:37,510 --> 00:17:39,570
And we&#39;re going to try to use those combination of features

441
00:17:39,570 --> 00:17:42,580
to predict the land use that we see here.

442
00:17:42,580 --> 00:17:43,890
Pixel by pixel, right?

443
00:17:43,890 --> 00:17:44,590
Pixel by pixel.

444
00:17:44,590 --> 00:17:46,720
Although we might want to use the fact that nearby pixels

445
00:17:46,720 --> 00:17:49,140
are more likely to be the same land use than ones

446
00:17:49,140 --> 00:17:50,710
that are far away.

447
00:17:50,710 --> 00:17:52,690
And we&#39;ll talk about classifiers.

448
00:17:52,690 --> 00:17:54,690
I think the one we use here is actually nearest neighbor, a

449
00:17:54,690 --> 00:17:55,580
very simple classifier.

450
00:17:55,580 --> 00:17:56,710
And that produces the prediction

451
00:17:56,710 --> 00:17:58,130
in the bottom right.

452
00:17:58,130 --> 00:17:58,990
And you can see it&#39;s quite good.

453
00:17:58,990 --> 00:17:59,750
It&#39;s not perfect.

454
00:17:59,750 --> 00:18:01,630
There&#39;s a few mistakes it makes.

455
00:18:01,630 --> 00:18:04,090
But it&#39;s, for the most part, quite accurate.

456
00:18:04,090 --> 00:18:04,380
OK.

457
00:18:04,380 --> 00:18:07,750
So that&#39;s the end of the series of examples.

458
00:18:07,750 --> 00:18:10,780
In the next session, we&#39;ll just tell you some notation

459
00:18:10,780 --> 00:18:14,600
and how we set up problems for supervised learning, which

460
00:18:14,600 --> 00:18:15,890
we&#39;ll use for the rest of the course.

