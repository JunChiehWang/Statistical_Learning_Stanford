0
00:00:00,920 --> 00:00:04,050
We&#39;re not going to go into more detail on multinomial

1
00:00:04,050 --> 00:00:05,000
regression now.

2
00:00:05,000 --> 00:00:06,940
What we&#39;re going to do is tell you about a different

3
00:00:06,940 --> 00:00:09,210
classification method, which is called discriminant

4
00:00:09,210 --> 00:00:11,950
analysis, which is also very useful.

5
00:00:11,950 --> 00:00:15,420
And it approaches a problem from a really quite different

6
00:00:15,420 --> 00:00:17,700
point of view.

7
00:00:17,700 --> 00:00:20,730
In discriminant analysis, the idea is to model the

8
00:00:20,730 --> 00:00:24,480
distribution of x in each of the classes separately.

9
00:00:24,480 --> 00:00:27,520
And then use what&#39;s known as Bayes theorem to flip things

10
00:00:27,520 --> 00:00:29,820
around to get the probability of y given x.

11
00:00:33,070 --> 00:00:36,610
In this case, for linear discriminant analysis, we&#39;re

12
00:00:36,610 --> 00:00:40,060
going to use Gaussian distributions for each class.

13
00:00:40,060 --> 00:00:42,260
And that&#39;s going to lead to linear or quadratic

14
00:00:42,260 --> 00:00:43,740
discriminant analysis.

15
00:00:43,740 --> 00:00:46,400
So those are the two popular forms.

16
00:00:46,400 --> 00:00:49,330
But as you&#39;ll see, this approach is quite general.

17
00:00:49,330 --> 00:00:52,210
And other distributions can be used as well.

18
00:00:52,210 --> 00:00:54,105
But we&#39;ll focus on normal distributions.

19
00:00:56,820 --> 00:00:59,820
So what is Bayes&#39; theorem for classification?

20
00:00:59,820 --> 00:01:02,790
It sounds pretty scary, but not too bad.

21
00:01:02,790 --> 00:01:06,070
So, of course, Thomas Bayes was a famous mathematician.

22
00:01:06,070 --> 00:01:09,190
And his name now, today, represents a burgeoning

23
00:01:09,190 --> 00:01:13,440
subfield of statistical and probabilistic modeling.

24
00:01:13,440 --> 00:01:15,830
But here we&#39;re going to focus on a very simple result which

25
00:01:15,830 --> 00:01:18,320
is known Bayes theorem.

26
00:01:18,320 --> 00:01:23,200
And it says that the probability of y equals k

27
00:01:23,200 --> 00:01:24,690
given x equals x.

28
00:01:24,690 --> 00:01:28,630
So the idea is you&#39;ve got two variables.

29
00:01:28,630 --> 00:01:31,170
In this case, we&#39;ve got y and x.

30
00:01:31,170 --> 00:01:35,300
And we&#39;re looking at aspects of their joint distribution.

31
00:01:35,300 --> 00:01:37,450
So this is what we&#39;re after, the probability of y

32
00:01:37,450 --> 00:01:39,950
equals k given x.

33
00:01:39,950 --> 00:01:43,710
And Bayes theorem says you can flip things around.

34
00:01:43,710 --> 00:01:46,820
You can write that as a probability that x is x given

35
00:01:46,820 --> 00:01:48,680
y equals k--

36
00:01:48,680 --> 00:01:52,160
that&#39;s the first piece on the top there--

37
00:01:52,160 --> 00:01:55,360
multiplied by the marginal probability or prior

38
00:01:55,360 --> 00:02:01,790
probability that y is k and then divided by the marginal

39
00:02:01,790 --> 00:02:04,200
probability that x equals x.

40
00:02:04,200 --> 00:02:07,820
So this is just a formula for probability theory.

41
00:02:07,820 --> 00:02:11,270
But it turns it&#39;s really useful and is a basis for

42
00:02:11,270 --> 00:02:14,420
discriminant analysis.

43
00:02:14,420 --> 00:02:17,880
And so we write things slightly differently in the

44
00:02:17,880 --> 00:02:20,750
case of discriminant analysis.

45
00:02:20,750 --> 00:02:28,490
So this probability y equals k is written as pi k.

46
00:02:28,490 --> 00:02:32,330
So if there&#39;s three classes, there&#39;s going to be three

47
00:02:32,330 --> 00:02:34,030
values for pi, just the probability

48
00:02:34,030 --> 00:02:35,270
for each of the classes.

49
00:02:35,270 --> 00:02:39,920
But here we&#39;ve got class little k, so that&#39;s pi k.

50
00:02:39,920 --> 00:02:45,870
Probability that x is x given y equals k, well, if x is a

51
00:02:45,870 --> 00:02:51,320
quantitative variable, what we write for that is the density.

52
00:02:51,320 --> 00:02:54,190
So that&#39;s a probability density function

53
00:02:54,190 --> 00:02:57,330
for x in class k.

54
00:02:57,330 --> 00:03:02,300
And then the marginal probability f of x is just

55
00:03:02,300 --> 00:03:04,440
this expression over here.

56
00:03:04,440 --> 00:03:07,210
So this is summing over all the classes.

57
00:03:10,840 --> 00:03:14,080
And so that&#39;s how we use Bayes theorem to get to the

58
00:03:14,080 --> 00:03:18,580
probabilities of interest, which is y equals k given x.

59
00:03:18,580 --> 00:03:20,260
Now, at this point it&#39;s still quite general.

60
00:03:20,260 --> 00:03:23,150
We can plug in any probability densities.

61
00:03:23,150 --> 00:03:25,390
But now what we&#39;re going to do is go ahead and plug in the

62
00:03:25,390 --> 00:03:29,270
Gaussian density for f sub-k of x.

63
00:03:33,110 --> 00:03:36,500
Before we do that, let me just show you a little picture to

64
00:03:36,500 --> 00:03:38,230
make things clear.

65
00:03:38,230 --> 00:03:40,420
In the left-hand plot, what have we got here?

66
00:03:40,420 --> 00:03:43,600
We&#39;ve got a plot against x, single variable x.

67
00:03:43,600 --> 00:03:49,940
And in the vertical axis, what we&#39;ve got is actually pi sub-k

68
00:03:49,940 --> 00:03:54,120
multiplied by f sub-k of x.

69
00:03:54,120 --> 00:03:58,050
For both classes, k equals 1 and k equals 2.

70
00:03:58,050 --> 00:04:01,790
Now, in this case, remember in the previous slide, the

71
00:04:01,790 --> 00:04:06,430
probability was essentially proportional to pi sub-k times

72
00:04:06,430 --> 00:04:09,300
f sub-k of x.

73
00:04:09,300 --> 00:04:12,180
And in this case, the pi&#39;s are the same for both.

74
00:04:12,180 --> 00:04:15,800
So it&#39;s really to do with which density is the highest.

75
00:04:15,800 --> 00:04:19,890
And you can see that the decision boundary, or the

76
00:04:19,890 --> 00:04:22,810
vertical dash line, is at zero.

77
00:04:22,810 --> 00:04:25,130
And that&#39;s the point at which the green density is higher

78
00:04:25,130 --> 00:04:27,370
than the purple density.

79
00:04:27,370 --> 00:04:32,600
And so anything to the left of zero we classify as green.

80
00:04:32,600 --> 00:04:37,330
And anything to the right we&#39;d classify as purple.

81
00:04:37,330 --> 00:04:41,680
And it sort of makes sense that that&#39;s what we do there.

82
00:04:41,680 --> 00:04:45,460
The right-hand plot has different priors.

83
00:04:45,460 --> 00:04:52,420
So here the probability of 2 is 0.7 and of 1 is 0.3.

84
00:04:52,420 --> 00:05:00,020
And now, again, we&#39;re plotting pi sub-k times f sub-k

85
00:05:00,020 --> 00:05:03,470
of x against x.

86
00:05:03,470 --> 00:05:06,930
And that big prior has bumped up the purple.

87
00:05:06,930 --> 00:05:09,810
And what it has done is move the decision boundary slightly

88
00:05:09,810 --> 00:05:11,360
to the left.

89
00:05:11,360 --> 00:05:12,230
And that makes sense, too.

90
00:05:12,230 --> 00:05:14,240
Again, that&#39;s where they intersect.

91
00:05:14,240 --> 00:05:16,640
That makes sense as well.

92
00:05:16,640 --> 00:05:19,270
Because we&#39;ve got more purples here.

93
00:05:19,270 --> 00:05:20,480
Actually, I&#39;ll say they are pinks.

94
00:05:20,480 --> 00:05:24,030
It looks purple to me.

95
00:05:24,030 --> 00:05:24,820
There&#39;s more of them.

96
00:05:24,820 --> 00:05:27,330
So everything else being equal, we&#39;re going to make

97
00:05:27,330 --> 00:05:33,260
less mistakes if we classify it to purples and to greens.

98
00:05:33,260 --> 00:05:38,290
So that&#39;s how these priors and the densities play a role in

99
00:05:38,290 --> 00:05:39,700
classification.

100
00:05:39,700 --> 00:05:41,600
So why discriminant analysis?

101
00:05:41,600 --> 00:05:43,210
It seems like logistic regression was

102
00:05:43,210 --> 00:05:44,550
a pretty good tool.

103
00:05:44,550 --> 00:05:45,660
Well, it is.

104
00:05:45,660 --> 00:05:47,210
But it turns out there&#39;s room for

105
00:05:47,210 --> 00:05:49,850
discriminant analysis as well.

106
00:05:49,850 --> 00:05:52,520
And so it&#39;s three points we make here.

107
00:05:52,520 --> 00:05:55,920
When the classes are well separated, it turns out that

108
00:05:55,920 --> 00:05:59,350
the parameter estimates for logistic regression are

109
00:05:59,350 --> 00:06:01,000
surprisingly unstable.

110
00:06:01,000 --> 00:06:03,610
In fact, if you&#39;ve go a feature that separates the

111
00:06:03,610 --> 00:06:05,280
classes perfectly, the

112
00:06:05,280 --> 00:06:08,500
coefficients go off to infinity.

113
00:06:08,500 --> 00:06:10,660
So it really doesn&#39;t do well there.

114
00:06:10,660 --> 00:06:14,480
Logistic aggression was developed in largely the

115
00:06:14,480 --> 00:06:17,970
biological and medical fields where you never found such

116
00:06:17,970 --> 00:06:20,860
strong predictors.

117
00:06:20,860 --> 00:06:24,900
Now, you can do things to make logistic

118
00:06:24,900 --> 00:06:26,260
regression better behave.

119
00:06:26,260 --> 00:06:28,860
But it turns out linear discriminant analysis doesn&#39;t

120
00:06:28,860 --> 00:06:31,600
suffer from this problem and is better behaved in those

121
00:06:31,600 --> 00:06:33,950
situations.

122
00:06:33,950 --> 00:06:37,210
Also, if any small, the sample size is small, and the

123
00:06:37,210 --> 00:06:39,670
distribution of the predictor&#39;s x is approximately

124
00:06:39,670 --> 00:06:44,610
normal in each of the classes, it turns out that discriminant

125
00:06:44,610 --> 00:06:49,050
model is again more stable than logistic regression.

126
00:06:49,050 --> 00:06:52,360
And finally, if we&#39;ve got more than two classes, we&#39;ll see

127
00:06:52,360 --> 00:06:54,730
logistic regression gives us nice low dimensions

128
00:06:54,730 --> 00:06:55,440
views of the data.

129
00:06:55,440 --> 00:06:57,830
And the other point, remember, in the very first section, we

130
00:06:57,830 --> 00:07:00,340
showed that the Bayes rule, if you have the right population

131
00:07:00,340 --> 00:07:03,690
model, Bayes rule is the best you can possibly do.

132
00:07:03,690 --> 00:07:07,420
So if our normal assumption is right here, then this

133
00:07:07,420 --> 00:07:10,190
discriminant analysis in the Bayes rule is the best you can

134
00:07:10,190 --> 00:07:11,080
possibly do.

135
00:07:11,080 --> 00:07:12,330
Good point, Rob.

