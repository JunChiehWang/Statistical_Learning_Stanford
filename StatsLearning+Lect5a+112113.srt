0
00:00:01,040 --> 00:00:02,660
Hello, everyone.

1
00:00:02,660 --> 00:00:03,925
We&#39;re going to continue now our discussion

2
00:00:03,925 --> 00:00:06,420
of supervised learning.

3
00:00:06,420 --> 00:00:09,050
Linear regression is the topic, and actually, as we&#39;ll

4
00:00:09,050 --> 00:00:10,240
see, it&#39;s a very simple method.

5
00:00:10,240 --> 00:00:11,590
But that&#39;s not a bad thing.

6
00:00:11,590 --> 00:00:13,940
Simple&#39;s actually good.

7
00:00:13,940 --> 00:00:16,650
As we&#39;ll see, it&#39;s very useful, and also the concepts

8
00:00:16,650 --> 00:00:19,940
we learned in linear regression are useful for a

9
00:00:19,940 --> 00:00:21,220
lot of the different topics in the course.

10
00:00:21,220 --> 00:00:23,280
So this is chapter three of our book.

11
00:00:23,280 --> 00:00:25,700
Let&#39;s look at the first slide.

12
00:00:25,700 --> 00:00:28,420
As we say, linear regression is a simpler approach to

13
00:00:28,420 --> 00:00:30,560
supervised learning that assumes the dependence of the

14
00:00:30,560 --> 00:00:34,780
outcome, Y, on the predictors, X1 through Xp, is linear.

15
00:00:34,780 --> 00:00:37,280
Now, let&#39;s look at that assumption.

16
00:00:37,280 --> 00:00:39,850
So in this little cartoon example, the true regression

17
00:00:39,850 --> 00:00:41,590
function is red.

18
00:00:41,590 --> 00:00:44,650
And it&#39;s not linear, but it&#39;s pretty close to linear.

19
00:00:44,650 --> 00:00:48,350
And the approximation in blue there, the blue line, it looks

20
00:00:48,350 --> 00:00:49,940
like a pretty good approximation.

21
00:00:49,940 --> 00:00:53,060
Especially if the noise around the true red curve, as we&#39;ll

22
00:00:53,060 --> 00:00:56,700
see, is substantial, the regression curve in blue could

23
00:00:56,700 --> 00:00:58,920
be quite a good approximation.

24
00:00:58,920 --> 00:01:02,740
So although this model is very simple--

25
00:01:02,740 --> 00:01:05,750
I think there&#39;s been sort of a tendency of people to think

26
00:01:05,750 --> 00:01:06,520
simple is bad.

27
00:01:06,520 --> 00:01:08,580
We want to use things that are complicated and fancy and

28
00:01:08,580 --> 00:01:09,050
impressive.

29
00:01:09,050 --> 00:01:11,570
Well, actually, I want to say the opposite.

30
00:01:11,570 --> 00:01:13,300
Simple is actually very good.

31
00:01:13,300 --> 00:01:15,540
And this model being very simple, it actually works

32
00:01:15,540 --> 00:01:18,740
extremely well in a lot of situations.

33
00:01:18,740 --> 00:01:22,410
And in addition, the concepts we learn in regression are

34
00:01:22,410 --> 00:01:25,210
important for a lot of the other supervised learning

35
00:01:25,210 --> 00:01:26,140
techniques in the course.

36
00:01:26,140 --> 00:01:28,930
So it&#39;s important to start slowly, to learn the concepts

37
00:01:28,930 --> 00:01:32,130
of the simple method, both for the method itself and for the

38
00:01:32,130 --> 00:01:34,510
future methods in the course.

39
00:01:34,510 --> 00:01:37,860
So what is the regression model?

40
00:01:37,860 --> 00:01:40,670
Well, before I define the model, let&#39;s actually look at

41
00:01:40,670 --> 00:01:43,700
the advertising data, which I&#39;ve got in the next slide.

42
00:01:43,700 --> 00:01:48,820
This data looks at sales as a function of three kinds of

43
00:01:48,820 --> 00:01:53,450
advertising, TV, radio, and newspaper.

44
00:01:53,450 --> 00:01:57,500
And here I&#39;ve got scatter plots of the sales versus each

45
00:01:57,500 --> 00:01:59,540
of the three predictors individually.

46
00:01:59,540 --> 00:02:03,730
And you can see the approximations by the

47
00:02:03,730 --> 00:02:05,080
regression line are pretty good.

48
00:02:05,080 --> 00:02:07,480
Looks like, for the most part, they&#39;re reasonable

49
00:02:07,480 --> 00:02:08,310
approximations.

50
00:02:08,310 --> 00:02:14,520
On the left side, maybe for low TV advertising, the sales

51
00:02:14,520 --> 00:02:16,420
are actually lower than expected,

52
00:02:16,420 --> 00:02:17,670
which we can see here.

53
00:02:20,220 --> 00:02:22,390
But for the most part, the linear approximation is

54
00:02:22,390 --> 00:02:24,660
reasonable, partly because, again, the amount of noise

55
00:02:24,660 --> 00:02:27,510
around the curve, around the line, is quite large.

56
00:02:27,510 --> 00:02:30,600
So even the actual regression function was nonlinear, we

57
00:02:30,600 --> 00:02:32,760
wouldn&#39;t be able to see it from this data.

58
00:02:32,760 --> 00:02:36,266
So this is an example of how it&#39;s this crude approximation,

59
00:02:36,266 --> 00:02:39,740
which is potentially quite useful.

60
00:02:39,740 --> 00:02:42,330
So what are the questions we might ask of this kind of

61
00:02:42,330 --> 00:02:44,770
data, and would you might ask the regression model to help

62
00:02:44,770 --> 00:02:45,900
us to answer?

63
00:02:45,900 --> 00:02:50,340
Well, one question is, is the relationship between the

64
00:02:50,340 --> 00:02:51,930
budget of advertising and sales.

65
00:02:51,930 --> 00:02:54,480
That&#39;s the sort of overall global question, is, do these

66
00:02:54,480 --> 00:02:57,810
predictors have anything to say about the outcome?

67
00:02:57,810 --> 00:02:59,870
Furthermore, how strong is that relationship?

68
00:02:59,870 --> 00:03:01,560
The relationship might be there, but it might be so weak

69
00:03:01,560 --> 00:03:04,120
as not to be useful.

70
00:03:04,120 --> 00:03:06,040
Now, assuming there is a relationship, which media

71
00:03:06,040 --> 00:03:08,030
contributed to sales?

72
00:03:08,030 --> 00:03:13,500
Is it TV, radio, or newspaper, or maybe all of them?

73
00:03:13,500 --> 00:03:16,150
If we want to use this model to predict, how well can we

74
00:03:16,150 --> 00:03:19,010
predict future sales?

75
00:03:19,010 --> 00:03:20,800
Is the relationship linear?

76
00:03:20,800 --> 00:03:22,660
We just discussed that already.

77
00:03:22,660 --> 00:03:24,860
If it&#39;s not linear, maybe if we use a nonlinear model,

78
00:03:24,860 --> 00:03:26,870
we&#39;ll be able to make better predictions.

79
00:03:26,870 --> 00:03:29,010
Is there synergy among the advertised media?

80
00:03:29,010 --> 00:03:32,610
In other words, do the media work on their own in a certain

81
00:03:32,610 --> 00:03:34,640
way, or do they work in combination?

82
00:03:34,640 --> 00:03:37,570
And we&#39;ll talk about ways of looking at synergy later in

83
00:03:37,570 --> 00:03:39,990
this section.

84
00:03:39,990 --> 00:03:41,870
OK, well, what is linear regression?

85
00:03:41,870 --> 00:03:45,000
Well, let&#39;s start with the simplest case, where a simple

86
00:03:45,000 --> 00:03:48,100
model with just a single predictor.

87
00:03:48,100 --> 00:03:49,330
And this is the model here.

88
00:03:49,330 --> 00:03:52,190
It says that the outcome is just a linear function of the

89
00:03:52,190 --> 00:03:55,640
single predictor, x, with noise, with errors,

90
00:03:55,640 --> 00:03:56,500
[INAUDIBLE] epsilon.

91
00:03:56,500 --> 00:03:59,090
So this is just the equation of a line.

92
00:03:59,090 --> 00:04:02,060
We&#39;ve added some noise at the end to allow the points to

93
00:04:02,060 --> 00:04:04,920
deviate from the line.

94
00:04:04,920 --> 00:04:09,000
The parameters that the constants, beta 0 and beta one

95
00:04:09,000 --> 00:04:11,570
are called parameters or coefficients.

96
00:04:11,570 --> 00:04:13,050
They&#39;re unknown.

97
00:04:13,050 --> 00:04:15,610
And we&#39;re going to find the best values to make the line

98
00:04:15,610 --> 00:04:17,260
fit as well as possible.

99
00:04:17,260 --> 00:04:20,070
So you see a lot terminology.

100
00:04:20,070 --> 00:04:23,340
Those parameters are called the intercept and slope,

101
00:04:23,340 --> 00:04:26,110
respectively, because they&#39;re the intercept and

102
00:04:26,110 --> 00:04:27,460
slope of the line.

103
00:04:27,460 --> 00:04:31,410
And again, we&#39;re going to find the best-fitting values to

104
00:04:31,410 --> 00:04:33,930
find the line that best fits the data.

105
00:04:33,930 --> 00:04:35,830
And we&#39;ll talk about that in the next slide.

106
00:04:35,830 --> 00:04:39,100
But suppose we have for the moment some good values for

107
00:04:39,100 --> 00:04:41,300
the slope and intercept.

108
00:04:41,300 --> 00:04:46,050
Then we can predict the future values simply by plugging them

109
00:04:46,050 --> 00:04:47,690
into the equation.

110
00:04:47,690 --> 00:04:50,730
So if we have a value of x, we want it for

111
00:04:50,730 --> 00:04:52,170
what you want to predict.

112
00:04:52,170 --> 00:04:54,605
The x might be, for example, the advertising

113
00:04:54,605 --> 00:04:56,690
you budget for TV.

114
00:04:56,690 --> 00:04:59,720
And we have our coefficients that we&#39;ve estimated.

115
00:04:59,720 --> 00:05:01,440
We simply plugged them into the equation, and our

116
00:05:01,440 --> 00:05:05,220
prediction for future sales at that value of x is given by

117
00:05:05,220 --> 00:05:06,950
this equation.

118
00:05:06,950 --> 00:05:09,650
And you&#39;ll see throughout the course, as is standard in

119
00:05:09,650 --> 00:05:13,990
statistics, we put a hat, this little symbol, over top of a

120
00:05:13,990 --> 00:05:18,000
parameter to indicate the estimated value which we&#39;ve

121
00:05:18,000 --> 00:05:19,680
estimated from data.

122
00:05:19,680 --> 00:05:21,230
So that&#39;s a sort of funny way.

123
00:05:21,230 --> 00:05:25,660
That&#39;s become a standard convention.

124
00:05:25,660 --> 00:05:29,980
So how do we find the best values of the parameters?

125
00:05:29,980 --> 00:05:34,840
Well, let&#39;s suppose we have the prediction for a given

126
00:05:34,840 --> 00:05:39,120
value of the parameters at each value in the data set.

127
00:05:39,120 --> 00:05:42,680
Then the residual, what&#39;s called the residual, is the

128
00:05:42,680 --> 00:05:44,675
discrepancy between the actual outcome and

129
00:05:44,675 --> 00:05:47,240
the predicted outcome.

130
00:05:47,240 --> 00:05:51,140
So we define the residual sum of squares as the total square

131
00:05:51,140 --> 00:05:56,466
discrepancy between the actual outcome and the fit.

132
00:05:56,466 --> 00:05:58,670
The equivalent, if you write that out in detail, it looks

133
00:05:58,670 --> 00:05:59,360
like this, right?

134
00:05:59,360 --> 00:06:02,380
This is the error, the residual for the first

135
00:06:02,380 --> 00:06:04,810
observation, square, second, et cetera.

136
00:06:04,810 --> 00:06:08,610
So it makes sense to say, well, I want to choose the

137
00:06:08,610 --> 00:06:10,890
values of these parameters, the intercept and slope, to

138
00:06:10,890 --> 00:06:12,570
make that as small as possible.

139
00:06:12,570 --> 00:06:15,080
In other words, I want the line to fit the points as

140
00:06:15,080 --> 00:06:17,216
closely as possible.

141
00:06:17,216 --> 00:06:18,600
Let&#39;s see.

142
00:06:18,600 --> 00:06:19,320
This next slide--

143
00:06:19,320 --> 00:06:21,170
I&#39;ll come back to the equation in the previous slide, but

144
00:06:21,170 --> 00:06:23,360
this next slide shows in pictures.

145
00:06:23,360 --> 00:06:25,080
So here are the points.

146
00:06:25,080 --> 00:06:29,320
Each of these residuals is the distance of each

147
00:06:29,320 --> 00:06:30,620
point from the line.

148
00:06:30,620 --> 00:06:32,620
And I square up these distances.

149
00:06:32,620 --> 00:06:34,370
I don&#39;t care if I&#39;m below or above.

150
00:06:34,370 --> 00:06:36,090
I&#39;m not going to give any preference.

151
00:06:36,090 --> 00:06:38,950
But I want the total score squared distance of all points

152
00:06:38,950 --> 00:06:41,070
to the line to be as small as possible.

153
00:06:41,070 --> 00:06:42,410
Because I want the line to be as close as

154
00:06:42,410 --> 00:06:44,270
possible to the points.

155
00:06:44,270 --> 00:06:46,490
This is called the least squares line.

156
00:06:46,490 --> 00:06:49,570
There&#39;s a unique line that fits the best in this sense.

157
00:06:49,570 --> 00:06:52,070
And the equations for the

158
00:06:52,070 --> 00:06:53,680
slope-intercept are given here.

159
00:06:53,680 --> 00:06:56,270
Here&#39;s the slope and the intercept.

160
00:06:56,270 --> 00:07:00,350
So just basically a formula involving the observations for

161
00:07:00,350 --> 00:07:03,160
the slope-intercept, and these [INAUDIBLE]

162
00:07:03,160 --> 00:07:05,350
the least squares estimates.

163
00:07:05,350 --> 00:07:07,170
These are the ones that minimize the sum of squares.

164
00:07:12,900 --> 00:07:15,660
Of course, a computer program like R or pretty much any

165
00:07:15,660 --> 00:07:18,650
other statistical program will compute that for you.

166
00:07:18,650 --> 00:07:21,130
You don&#39;t need to do it by hand.

167
00:07:21,130 --> 00:07:23,680
OK, so we have our data for a single predictor.

168
00:07:23,680 --> 00:07:25,930
We&#39;ve obtained the least squares estimates.

169
00:07:25,930 --> 00:07:28,400
Well, one question we want to know is how

170
00:07:28,400 --> 00:07:30,860
precise are those estimates.

171
00:07:30,860 --> 00:07:32,110
In particular, we want to know what?

172
00:07:32,110 --> 00:07:33,780
We want to know, for example, is the slope 0?

173
00:07:33,780 --> 00:07:36,160
If the slope is 0, that means there&#39;s no relationship

174
00:07:36,160 --> 00:07:37,750
between y and x.

175
00:07:37,750 --> 00:07:40,300
Suppose we obtained a slope of 0.5.

176
00:07:40,300 --> 00:07:42,920
Is that bigger than 0 or not?

177
00:07:42,920 --> 00:07:44,770
Well, we need a measure of precision.

178
00:07:44,770 --> 00:07:48,180
How close is that actually to 0?

179
00:07:48,180 --> 00:07:52,320
Maybe if we got a new dataset from the same population, we

180
00:07:52,320 --> 00:07:54,165
get a slope of minus 0.1.

181
00:07:54,165 --> 00:07:57,680
Then the 0.5 is not as impressive as it sounds.

182
00:07:57,680 --> 00:08:00,585
So we need what&#39;s called the standard error for the slope

183
00:08:00,585 --> 00:08:01,450
and intercept.

184
00:08:01,450 --> 00:08:04,150
Well, here are the formulas for the standard errors of the

185
00:08:04,150 --> 00:08:05,490
slope and intercept.

186
00:08:05,490 --> 00:08:06,580
Here&#39;s the one we really care about.

187
00:08:06,580 --> 00:08:09,390
This is the square standard error of the slope.

188
00:08:09,390 --> 00:08:12,220
It&#39;s sigma squared, where sigma squared is the noise,

189
00:08:12,220 --> 00:08:19,680
the variance of the errors around the line.

190
00:08:19,680 --> 00:08:22,600
And this is interesting.

191
00:08:22,600 --> 00:08:25,690
It says this is the spread of the x&#39;s around their mean.

192
00:08:25,690 --> 00:08:26,950
This actually makes sense.

193
00:08:26,950 --> 00:08:30,020
It says, the standard error of the slope is bigger if my

194
00:08:30,020 --> 00:08:31,520
noise variance is bigger.

195
00:08:31,520 --> 00:08:32,150
That makes sense.

196
00:08:32,150 --> 00:08:33,490
The more noise around the line, the

197
00:08:33,490 --> 00:08:35,200
less precise the slope.

198
00:08:35,200 --> 00:08:38,570
This says the more spread out the x&#39;s, the more

199
00:08:38,570 --> 00:08:40,770
precise the slope is.

200
00:08:40,770 --> 00:08:41,809
And that actually makes sense.

201
00:08:41,808 --> 00:08:44,530
I go back to this slide.

202
00:08:44,530 --> 00:08:47,240
The more spread out these points are, the more I have

203
00:08:47,240 --> 00:08:49,580
the slope pinned down.

204
00:08:49,580 --> 00:08:51,140
Think of like a teeter totter.

205
00:08:51,140 --> 00:08:54,880
Imagine I had the points, they were all actually concentrated

206
00:08:54,880 --> 00:08:56,500
around 150.

207
00:08:56,500 --> 00:08:58,010
Then this slope could vary a lot.

208
00:09:02,380 --> 00:09:05,610
I could turn it, change the slope, and still fit the

209
00:09:05,610 --> 00:09:07,000
points about the same.

210
00:09:07,000 --> 00:09:10,370
But the more the points are spread out in x across the

211
00:09:10,370 --> 00:09:13,685
horizontal axis, the better pinned down I have the slope,

212
00:09:13,685 --> 00:09:17,730
the less slop it has to turn.

213
00:09:17,730 --> 00:09:21,650
So this also says you have a choice of which observations

214
00:09:21,650 --> 00:09:23,840
to measure.

215
00:09:23,840 --> 00:09:27,000
And so maybe in an experiment where you can design, you

216
00:09:27,000 --> 00:09:29,450
should pick your predictor values, the x&#39;s, as spread out

217
00:09:29,450 --> 00:09:32,420
as possible in order to get the slopes estimated as

218
00:09:32,420 --> 00:09:35,950
precisely as possible.

219
00:09:35,950 --> 00:09:39,320
OK So that&#39;s the formula for the standard error of the

220
00:09:39,320 --> 00:09:42,610
slope and for the intercept.

221
00:09:42,610 --> 00:09:43,670
And what we do with these?

222
00:09:43,670 --> 00:09:45,490
Well, one thing we can do is form what&#39;s

223
00:09:45,490 --> 00:09:47,430
called confidence intervals.

224
00:09:47,430 --> 00:09:55,000
So a confidence interval is defined as a range so that it

225
00:09:55,000 --> 00:09:58,935
has a property that with high confidence, 95%, say, which is

226
00:09:58,935 --> 00:10:03,220
the number that we&#39;ll pick, that that range contains the

227
00:10:03,220 --> 00:10:06,980
true value with that confidence.

228
00:10:06,980 --> 00:10:10,360
In other words, to be specific, if you want a

229
00:10:10,360 --> 00:10:14,520
confidence interval of 95%, we take the estimate of our slope

230
00:10:14,520 --> 00:10:16,110
plus or minus twice the estimate of

231
00:10:16,110 --> 00:10:17,460
the standard error.

232
00:10:17,460 --> 00:10:21,100
And this, if errors are normally distributed, which we

233
00:10:21,100 --> 00:10:23,680
typically assume, approximately, this will

234
00:10:23,680 --> 00:10:26,140
contain the true value, the true slope,

235
00:10:26,140 --> 00:10:32,060
with probability 0.95.

236
00:10:32,060 --> 00:10:34,400
OK, so what we get from that is a confidence interval,

237
00:10:34,400 --> 00:10:38,770
which is a lower point and an upper point, which contains

238
00:10:38,770 --> 00:10:41,510
the true value with probability 0.95 under

239
00:10:41,510 --> 00:10:43,530
repeated sampling.

240
00:10:43,530 --> 00:10:44,260
Now, what does that mean?

241
00:10:44,260 --> 00:10:45,920
[INAUDIBLE] a little tricky to interpret that.

242
00:10:45,920 --> 00:10:47,870
Let&#39;s see in a little more detail what

243
00:10:47,870 --> 00:10:49,220
that actually means.

244
00:10:49,220 --> 00:10:53,500
Let&#39;s think of a true value of beta, beta one, which might be

245
00:10:53,500 --> 00:10:56,630
0 in particular, which means the slope is 0.

246
00:10:56,630 --> 00:10:59,590
And now let&#39;s draw a line at beta one.

247
00:10:59,590 --> 00:11:02,780
Now imagine that we draw a dataset like the one we drew,

248
00:11:02,780 --> 00:11:05,350
and we get a confidence interval from this formula,

249
00:11:05,350 --> 00:11:08,080
and that conference interval looks like this.

250
00:11:08,080 --> 00:11:10,700
So this one contains a true value because they&#39;ve got the

251
00:11:10,700 --> 00:11:12,725
line is in between in the bracket.

252
00:11:12,725 --> 00:11:15,890
Now I get a second dataset from the same population, and

253
00:11:15,890 --> 00:11:20,000
I form this confidence interval from that dataset.

254
00:11:20,000 --> 00:11:21,590
It looks a little different, but it also

255
00:11:21,590 --> 00:11:23,460
contains a true value.

256
00:11:23,460 --> 00:11:26,480
Now I get a third data set, and I do the least squares

257
00:11:26,480 --> 00:11:27,120
computation.

258
00:11:27,120 --> 00:11:29,420
I form the confidence interval.

259
00:11:29,420 --> 00:11:31,930
Unluckily, it doesn&#39;t contain the true value.

260
00:11:31,930 --> 00:11:32,610
It&#39;s sitting over here.

261
00:11:32,610 --> 00:11:34,460
It&#39;s above beta one.

262
00:11:34,460 --> 00:11:36,720
Beta one&#39;s below the whole interval.

263
00:11:36,720 --> 00:11:37,710
And I get another dataset.

264
00:11:37,710 --> 00:11:41,480
Maybe I miss it on the other side this time.

265
00:11:41,480 --> 00:11:45,120
And I get another dataset, and I contain the true value.

266
00:11:45,120 --> 00:11:48,080
So we can imagine doing this experiment many, many times,

267
00:11:48,080 --> 00:11:50,790
each time getting a new dataset from the population,

268
00:11:50,790 --> 00:11:52,980
doing least squares computation, and forming the

269
00:11:52,980 --> 00:11:54,490
confidence interval.

270
00:11:54,490 --> 00:11:59,320
And what the theory tells us is that if I form, say, 100

271
00:11:59,320 --> 00:12:05,220
confidence intervals, 100 of these brackets, 95% of the

272
00:12:05,220 --> 00:12:07,460
time, they will contain the true value.

273
00:12:07,460 --> 00:12:09,180
The other 5% of the time, they will not

274
00:12:09,180 --> 00:12:10,940
contain the true value.

275
00:12:10,940 --> 00:12:15,190
So I can be pretty sure that the interval contains the true

276
00:12:15,190 --> 00:12:18,160
value if I form the confidence interval in this way.

277
00:12:18,160 --> 00:12:23,180
I can be sure at probability 0.95.

278
00:12:23,180 --> 00:12:26,470
So for the advertising data, the confidence interval for

279
00:12:26,470 --> 00:12:31,400
beta one is 0.042 to 0.053.

280
00:12:31,400 --> 00:12:34,730
This is for TV sales.

281
00:12:34,730 --> 00:12:42,620
So this tells me that the true slope for TV advertising is--

282
00:12:42,620 --> 00:12:45,180
first of all, it&#39;s greater than 0.

283
00:12:45,180 --> 00:12:47,160
In other words, having TV advertising does have a

284
00:12:47,160 --> 00:12:52,170
positive effect on sales, as one would expect.

285
00:12:52,170 --> 00:12:54,300
OK, so that completes our discussion of standard errors

286
00:12:54,300 --> 00:12:55,520
and confidence intervals.

287
00:12:55,520 --> 00:12:57,490
In the next segment, we&#39;ll talk about hypothesis testing,

288
00:12:57,490 --> 00:12:59,770
which is a closely related idea to confidence intervals.

