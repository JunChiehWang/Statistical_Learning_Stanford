0
00:00:00,900 --> 00:00:05,470
OK, here we have a session in R, in R Studio in fact, and

1
00:00:05,470 --> 00:00:08,220
we&#39;re going to see how we fit the logistic regression models

2
00:00:08,220 --> 00:00:13,370
using the GLM functions in R. So what we first begin to do

3
00:00:13,370 --> 00:00:18,530
is load the ISLR package, which has all the datasets

4
00:00:18,530 --> 00:00:20,140
we&#39;re going to use.

5
00:00:20,140 --> 00:00:22,140
Here we use the command require, which

6
00:00:22,140 --> 00:00:24,290
is similar to library.

7
00:00:24,290 --> 00:00:26,960
I tend to use require.

8
00:00:26,960 --> 00:00:31,000
It&#39;s sort of more evocative, what we&#39;re doing.

9
00:00:31,000 --> 00:00:37,650
And so we just click through in R Studio, and the Smarket

10
00:00:37,650 --> 00:00:39,840
dataset&#39;s there, and we can query it.

11
00:00:39,840 --> 00:00:44,180
So names is useful for seeing what&#39;s on the data frame, and

12
00:00:44,180 --> 00:00:45,980
summary is also useful.

13
00:00:45,980 --> 00:00:49,110
Summary gives you a simple summary of each of the

14
00:00:49,110 --> 00:00:52,480
variables on the Smarket data frame.

15
00:00:52,480 --> 00:00:56,680
And we see that there&#39;s a number of lags, and there&#39;s

16
00:00:56,680 --> 00:01:01,690
volume, and there&#39;s today&#39;s price, and there&#39;s direction,

17
00:01:01,690 --> 00:01:04,269
which we&#39;re going to use as a response, and that is whether

18
00:01:04,269 --> 00:01:08,510
the market went up or down since the previous day.

19
00:01:08,510 --> 00:01:13,590
And we can also do help on these data objects and get

20
00:01:13,590 --> 00:01:15,880
some details of each of the variables.

21
00:01:15,880 --> 00:01:18,180
And so there you see the description.

22
00:01:18,180 --> 00:01:21,040
So we&#39;re going to use the direction as a response and

23
00:01:21,040 --> 00:01:25,520
see if we can predict it as a binary response using logistic

24
00:01:25,520 --> 00:01:27,040
regression.

25
00:01:27,040 --> 00:01:28,420
Let&#39;s make a plot of our data.

26
00:01:28,420 --> 00:01:30,510
So there&#39;s the pairs function.

27
00:01:30,510 --> 00:01:38,710
And we do it, it&#39;s a little bit crowded in this plot.

28
00:01:38,710 --> 00:01:45,170
We told it to plot the variables in Smarket.

29
00:01:45,170 --> 00:01:50,150
And we told it to use as the color indicator, actually our

30
00:01:50,150 --> 00:01:51,060
binary response.

31
00:01:51,060 --> 00:01:57,900
And that&#39;s a useful way, when you&#39;ve got a two-class

32
00:01:57,900 --> 00:02:02,270
variable for seeing in the plots which are members of

33
00:02:02,270 --> 00:02:03,760
each class.

34
00:02:03,760 --> 00:02:06,290
And here we see them.

35
00:02:06,290 --> 00:02:10,639
Doesn&#39;t seem to be too much correlation going on here.

36
00:02:10,639 --> 00:02:13,540
Of course, the class variable is derived from the variable

37
00:02:13,540 --> 00:02:18,940
today, and so up or down seems to make a division.

38
00:02:18,940 --> 00:02:23,710
And other than that, we don&#39;t see much going on.

39
00:02:23,710 --> 00:02:26,100
Of course, with stock market data, you don&#39;t expect to see

40
00:02:26,100 --> 00:02:27,850
much going on.

41
00:02:27,850 --> 00:02:31,470
Because if things were very easy to predict, people would

42
00:02:31,470 --> 00:02:33,450
be making lots of money from it, and they wouldn&#39;t be

43
00:02:33,450 --> 00:02:37,620
giving lectures on using R.

44
00:02:37,620 --> 00:02:41,905
So here we have a call to GLM, glm.fit.

45
00:02:41,905 --> 00:02:45,640
We tell it direction is the response, and the predictors,

46
00:02:45,640 --> 00:02:49,200
we&#39;re going to use the lag variables, so that yesterday&#39;s

47
00:02:49,200 --> 00:02:53,190
price, previous day&#39;s price, and so on, up to lag5.

48
00:02:53,190 --> 00:02:56,760
And we&#39;ll also use volume, and we tell it to use family

49
00:02:56,760 --> 00:02:57,760
equals binomial.

50
00:02:57,760 --> 00:03:00,630
And so that tells GLM to put fit a logistic regression

51
00:03:00,630 --> 00:03:03,710
model instead of one of the many other models that can be

52
00:03:03,710 --> 00:03:05,470
fit to the GLM.

53
00:03:05,470 --> 00:03:08,500
So we execute that fit.

54
00:03:08,500 --> 00:03:10,590
And oops.

55
00:03:10,590 --> 00:03:12,350
I skipped a line here.

56
00:03:12,350 --> 00:03:13,330
So there we go.

57
00:03:13,330 --> 00:03:14,780
And there, it&#39;s done.

58
00:03:14,780 --> 00:03:16,070
That&#39;s two lines there.

59
00:03:16,070 --> 00:03:20,610
And then we do a summary, and the summary tells you some

60
00:03:20,610 --> 00:03:22,610
things about the fit.

61
00:03:22,610 --> 00:03:26,430
And in this case, it&#39;s giving you p values on each of the

62
00:03:26,430 --> 00:03:27,060
coefficients.

63
00:03:27,060 --> 00:03:29,790
It estimated coefficients, their standard errors, the

64
00:03:29,790 --> 00:03:31,730
z-score, and the p value.

65
00:03:31,730 --> 00:03:33,610
And it seems like none of the coefficients

66
00:03:33,610 --> 00:03:36,600
are significant here.

67
00:03:36,600 --> 00:03:40,690
Again, not a big surprise for these kinds of data.

68
00:03:40,690 --> 00:03:45,840
It doesn&#39;t necessarily mean it won&#39;t be able to make any kind

69
00:03:45,840 --> 00:03:47,010
of reasonable predictions.

70
00:03:47,010 --> 00:03:49,490
It just means that possibly these variables are very

71
00:03:49,490 --> 00:03:50,600
correlated.

72
00:03:50,600 --> 00:03:53,030
Actually, the plot doesn&#39;t suggest that.

73
00:03:53,030 --> 00:03:55,200
Anyway, none are significant.

74
00:03:55,200 --> 00:03:57,070
And it gives the null deviance, which is the

75
00:03:57,070 --> 00:03:59,070
deviance just for the mean.

76
00:03:59,070 --> 00:04:01,180
So that&#39;s the log likelihood if you just use the mean

77
00:04:01,180 --> 00:04:04,960
model, and then the deviance for the model with all the

78
00:04:04,960 --> 00:04:08,330
predictors in, that&#39;s the residual deviance.

79
00:04:08,330 --> 00:04:11,270
And there was a very modest change in deviance.

80
00:04:11,270 --> 00:04:17,769
It looks like four units on how many degrees of freedom.

81
00:04:17,769 --> 00:04:20,899
We got six degrees of freedom difference in degrees of

82
00:04:20,899 --> 00:04:22,550
freedom there.

83
00:04:22,550 --> 00:04:29,410
OK, so we can make predictions from the fitted model.

84
00:04:29,410 --> 00:04:37,650
And so we assign to glm.probs the predict of glm.fit, and we

85
00:04:37,650 --> 00:04:39,570
tell it type equals response.

86
00:04:39,570 --> 00:04:42,220
So this will make predictions on the training data that we

87
00:04:42,220 --> 00:04:43,920
use to fit the model.

88
00:04:43,920 --> 00:04:48,270
And it gives you a vector of fitted probabilities.

89
00:04:48,270 --> 00:04:52,050
We can look at the first five, and we see that they&#39;re very

90
00:04:52,050 --> 00:04:56,490
close to 50%, which is, again, not too surprising.

91
00:04:56,490 --> 00:04:59,610
We don&#39;t expect to get strong predictions in this case.

92
00:04:59,610 --> 00:05:03,570
So this is a prediction of whether the market&#39;s going to

93
00:05:03,570 --> 00:05:07,210
be up or down based on the lags and the other predictors.

94
00:05:12,030 --> 00:05:16,470
We can turn those probabilities into

95
00:05:16,470 --> 00:05:19,610
classifications by thresholding at 0.5.

96
00:05:19,610 --> 00:05:22,660
And so we do that by using the if/else command.

97
00:05:22,660 --> 00:05:28,370
So if/else takes effect, in this case, glm.probs, in fact,

98
00:05:28,370 --> 00:05:30,120
a vector of logicals.

99
00:05:30,120 --> 00:05:33,090
So glm.probs bigger than 0.5.

100
00:05:33,090 --> 00:05:36,530
So that&#39;ll be a vector of trues and falses.

101
00:05:36,530 --> 00:05:41,220
And then if/else says that, element by element, if it&#39;s

102
00:05:41,220 --> 00:05:42,780
true, you&#39;re going to call it up.

103
00:05:42,780 --> 00:05:45,400
Otherwise, you&#39;re going to call it down.

104
00:05:45,400 --> 00:05:47,690
And so that does that for us.

105
00:05:47,690 --> 00:05:51,080
And now we&#39;re going to look at our performance.

106
00:05:51,080 --> 00:05:54,250
And it&#39;s convenient to actually attach the data frame

107
00:05:54,250 --> 00:05:56,440
market so that the variables are available by

108
00:05:56,440 --> 00:05:58,400
name, which we do.

109
00:05:58,400 --> 00:06:03,170
And now we can make a table of glm.pred, which is our ups and

110
00:06:03,170 --> 00:06:07,210
downs from our prediction, against the true direction.

111
00:06:07,210 --> 00:06:08,760
So we do that.

112
00:06:08,760 --> 00:06:12,660
And we get a table, and we see there&#39;s lots of elements on

113
00:06:12,660 --> 00:06:13,910
the off diagonals.

114
00:06:15,780 --> 00:06:20,400
On the diagonals is where we do correct classification, and

115
00:06:20,400 --> 00:06:23,580
on the off diagonals is where we make mistakes.

116
00:06:23,580 --> 00:06:25,300
And we see there&#39;s quite a lot of those.

117
00:06:25,300 --> 00:06:29,960
And we can actually get our mean classification

118
00:06:29,960 --> 00:06:31,050
performance.

119
00:06:31,050 --> 00:06:35,420
So that&#39;s cases where glm.pred is equal to the direction.

120
00:06:35,420 --> 00:06:38,040
And we just take the mean of those, so it&#39;ll give you a

121
00:06:38,040 --> 00:06:41,060
proportion, in this case, 0.52.

122
00:06:41,060 --> 00:06:46,240
So on the training data, we do slightly better than chance.

123
00:06:46,240 --> 00:06:49,270
Well, we may have overfit on the training data.

124
00:06:49,270 --> 00:06:52,120
So what we&#39;re going to do now is divide our data up into a

125
00:06:52,120 --> 00:06:55,620
training and test set.

126
00:06:55,620 --> 00:07:01,510
So what we&#39;ll do is we&#39;ll make a vector of logicals.

127
00:07:01,510 --> 00:07:07,700
And what it is is train is equal to year less than 2005.

128
00:07:07,700 --> 00:07:11,030
For all those observations for which year is less than 2005,

129
00:07:11,030 --> 00:07:12,120
we&#39;ll get a true.

130
00:07:12,120 --> 00:07:13,370
Otherwise, we&#39;ll get a false.

131
00:07:15,850 --> 00:07:20,670
And now we refit our glm.fit, except we say

132
00:07:20,670 --> 00:07:22,200
subset equals train.

133
00:07:22,200 --> 00:07:24,610
And so it will use any those observations for

134
00:07:24,610 --> 00:07:26,210
which train is true.

135
00:07:26,210 --> 00:07:30,690
So now that means we fit just to the data in

136
00:07:30,690 --> 00:07:33,670
years less than 2005.

137
00:07:33,670 --> 00:07:36,230
And now, when we come to predict, we&#39;re going to

138
00:07:36,230 --> 00:07:40,370
predict on the remaining data, which is

139
00:07:40,370 --> 00:07:42,810
years 2005 or greater.

140
00:07:42,810 --> 00:07:45,980
And so we use the predict function again.

141
00:07:45,980 --> 00:07:48,915
And for the new data, we give it Smarket, but

142
00:07:48,915 --> 00:07:50,050
index by not trained.

143
00:07:50,050 --> 00:07:51,570
So that&#39;s going to be--

144
00:07:51,570 --> 00:07:59,510
not trained is going to be true if year is 2005 or more.

145
00:07:59,510 --> 00:08:02,720
And we tell it type equals response, so we actually want

146
00:08:02,720 --> 00:08:08,400
to predict the probabilities in this case.

147
00:08:08,400 --> 00:08:11,390
And again, we make this if/else, make

148
00:08:11,390 --> 00:08:13,670
this up/down variable.

149
00:08:13,670 --> 00:08:21,670
And let&#39;s make a subset, a new variable, direction.2005,

150
00:08:21,670 --> 00:08:24,750
again, for the test data, which is the response

151
00:08:24,750 --> 00:08:29,040
variable, direction, which is just for our test data.

152
00:08:29,040 --> 00:08:30,280
In other words, not trained.

153
00:08:30,280 --> 00:08:33,539
So we call it direction.2005.

154
00:08:33,539 --> 00:08:35,150
And now we make a table.

155
00:08:35,150 --> 00:08:36,860
So now this is on test data.

156
00:08:36,860 --> 00:08:39,360
A smaller subset of the data is test.

157
00:08:39,360 --> 00:08:42,830
And we compute our mean again, and it&#39;s 0.48.

158
00:08:42,830 --> 00:08:48,780
So now we actually get slightly less than 50%.

159
00:08:48,780 --> 00:08:55,330
So we&#39;re doing worse than the null rate, which is 50%.

160
00:08:55,330 --> 00:08:57,120
So what are we going to do?

161
00:08:57,120 --> 00:08:59,790
Well, we might be overfitting.

162
00:08:59,790 --> 00:09:03,650
And that&#39;s why we&#39;re doing worse on the test data.

163
00:09:03,650 --> 00:09:05,620
So now we&#39;re going to fit a smaller model.

164
00:09:05,620 --> 00:09:09,820
So we&#39;re going to just use lag1 and lag2 and leave out

165
00:09:09,820 --> 00:09:11,430
all the other variables.

166
00:09:11,430 --> 00:09:13,610
And so here we do that.

167
00:09:13,610 --> 00:09:15,910
The rest of it calls the same.

168
00:09:15,910 --> 00:09:19,850
And then we run through our commands again, we compute our

169
00:09:19,850 --> 00:09:28,020
table, and here we get a correct classification of just

170
00:09:28,020 --> 00:09:32,290
about 56%, which is not too bad at all.

171
00:09:32,290 --> 00:09:38,250
And so using the smaller model, it appears to have done

172
00:09:38,250 --> 00:09:39,310
better here.

173
00:09:39,310 --> 00:09:43,900
And if we do a summary of that guy, let&#39;s see if anything

174
00:09:43,900 --> 00:09:50,380
became significant by using the smaller model, given that

175
00:09:50,380 --> 00:09:53,130
it gave us better predictions.

176
00:09:53,130 --> 00:09:55,270
And of course, that&#39;s what happens when you try and do

177
00:09:55,270 --> 00:09:56,290
commands on the--

178
00:09:56,290 --> 00:09:58,270
type on the fly, at least for me, you

179
00:09:58,270 --> 00:09:59,920
make spelling mistakes.

180
00:09:59,920 --> 00:10:02,630
Well, nothing became significant, but at least the

181
00:10:02,630 --> 00:10:05,700
prediction of performance appeared to have increased.

182
00:10:05,700 --> 00:10:08,710
So that&#39;s fitting logistic regression models in R using

183
00:10:08,710 --> 00:10:11,390
the GLM function and family equals binomial.

